{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Heterogeneous-Memory Storage Engine HSE is a fast embeddable key-value store designed for SSDs and persistent memory. HSE optimizes performance and endurance by orchestrating data placement across DRAM and multiple classes of solid-state storage devices. HSE is ideal for powering NoSQL, Software-Defined Storage (SDS), High-Performance Computing (HPC), Big Data, Internet of Things (IoT), and Machine Learning (ML) solutions. Key Features Standard and advanced key-value operators Full transactions with snapshot-isolation spanning multiple independent key-value collections Cursors for iterating over snapshot views Data model for optimizing mixed use-case workloads in a single data store Key and value compression Flexible durability controls Configurable data orchestration schemes Native C API library that can be embedded in any application Benefits Scales to terabytes of data and hundreds of billions of keys per store Efficiently handles thousands of concurrent operations Dramatically improves throughput, latency, write-amplification, and read-amplification versus common alternatives for many workloads Optionally combines multiple classes of solid-state storage to optimize performance and endurance Performance HSE delivers up to 6x the throughput of RocksDB on the industry standard YCSB benchmark. HSE achieves this higher throughput with lower latency, write-amp, and read-amp as detailed here . HSE integrated with MongoDB\u00ae delivers up to 8x the YCSB throughput of the default WiredTiger storage engine. Here again, HSE achieves this higher throughput with lower latency, write-amp, and read-amp as detailed here .","title":"Home"},{"location":"#heterogeneous-memory-storage-engine","text":"HSE is a fast embeddable key-value store designed for SSDs and persistent memory. HSE optimizes performance and endurance by orchestrating data placement across DRAM and multiple classes of solid-state storage devices. HSE is ideal for powering NoSQL, Software-Defined Storage (SDS), High-Performance Computing (HPC), Big Data, Internet of Things (IoT), and Machine Learning (ML) solutions.","title":"Heterogeneous-Memory Storage Engine"},{"location":"#key-features","text":"Standard and advanced key-value operators Full transactions with snapshot-isolation spanning multiple independent key-value collections Cursors for iterating over snapshot views Data model for optimizing mixed use-case workloads in a single data store Key and value compression Flexible durability controls Configurable data orchestration schemes Native C API library that can be embedded in any application","title":"Key Features"},{"location":"#benefits","text":"Scales to terabytes of data and hundreds of billions of keys per store Efficiently handles thousands of concurrent operations Dramatically improves throughput, latency, write-amplification, and read-amplification versus common alternatives for many workloads Optionally combines multiple classes of solid-state storage to optimize performance and endurance","title":"Benefits"},{"location":"#performance","text":"HSE delivers up to 6x the throughput of RocksDB on the industry standard YCSB benchmark. HSE achieves this higher throughput with lower latency, write-amp, and read-amp as detailed here . HSE integrated with MongoDB\u00ae delivers up to 8x the YCSB throughput of the default WiredTiger storage engine. Here again, HSE achieves this higher throughput with lower latency, write-amp, and read-amp as detailed here .","title":"Performance"},{"location":"apps/mongodb/","text":"MongoDB MongoDB\u00ae is a popular document-oriented NoSQL database. The MongoDB implementation is open source, and includes an extensible framework for integrating different storage engines. We integrated HSE with MongoDB 3.4.17 to validate its benefits within a real-world storage application. The hse-mongo repo is a fork of MongoDB that adds support for HSE. MongoDB with HSE (MongoDB/HSE) demonstrates significantly increased performance and scalability for many workloads when storing data on SSDs. In the sections that follow, the reader is assumed to be familiar with configuring and running MongoDB. The information provided here is specific to using MongoDB with HSE. Install HSE and Create a KVDB Review the getting started section of this documentation. Then, following those instructions and examples, start by Installing HSE and mpool from packages or source Configuring an mpool for HSE storage with appropriate permissions Creating an HSE KVDB in that mpool to store the MongoDB data In the mongod.conf example below, we assume the name of the mpool is mongoData , which is also the name of the KVDB. Install MongoDB Dependencies Install dependencies for your platform. RHEL 8 $ sudo dnf install python2-pip gcc-toolset-9 libuuid-devel lz4 lz4-devel openssl-devel openssl-devel numactl libpcap libpcap-devel golang-1.11.13 createrepo rpmdevtools $ sudo alternatives --set python /usr/bin/python2 $ pip2 install --user scons Ubuntu 18.04 $ sudo apt-get install debhelper rpm golang libpcap-dev RHEL 7 $ sudo yum install devtoolset-9 rh-mongodb32-scons.noarch scl-utils libuuid-devel lz4 lz4-devel openssl-devel numactl libpcap libpcap-devel golang-1.9.4 python36 createrepo rpmdevtools Install MongoDB with HSE from Packages MongoDB/HSE can be installed from release packages. Download and install the latest hse-mongo packages for your platform. Package names start with hse-mongodb-*A.B.C.D.E-X.Y.Z , where A.B.C is the MongoDB version (e.g., 3.4.17 ) D.E is our MongoDB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific prefix is hse-mongodb-server-3.4.17.2.1-1.8.0. Install the packages for your platform as follows. RHEL 8 $ sudo dnf install ./hse-mongodb-server-A.B.C.D.E-X.Y.Z*.rpm $ sudo dnf install ./hse-mongodb-mongos-A.B.C.D.E-X.Y.Z*.rpm $ sudo dnf install ./hse-mongodb-shell-A.B.C.D.E-X.Y.Z*.rpm $ sudo dnf install ./hse-mongodb-tools-A.B.C.D.E-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./hse-mongodb-server_A.B.C.D.E-X.Y.Z*.deb $ sudo apt-get install ./hse-mongodb-mongos_A.B.C.D.E-X.Y.Z*.deb $ sudo apt-get install ./hse-mongodb-shell_A.B.C.D.E-X.Y.Z*.deb $ sudo apt-get install ./hse-mongodb-tools_A.B.C.D.E-X.Y.Z*.deb RHEL 7 $ sudo yum install ./hse-mongodb-server-A.B.C.D.E-X.Y.Z*.rpm $ sudo yum install ./hse-mongodb-mongos-A.B.C.D.E-X.Y.Z*.rpm $ sudo yum install ./hse-mongodb-shell-A.B.C.D.E-X.Y.Z*.rpm $ sudo yum install ./hse-mongodb-tools-A.B.C.D.E-X.Y.Z*.rpm Install MongoDB with HSE from Source MongoDB/HSE can also be built and installed from source. Clone the latest release tag from the hse-mongo repo . Releases are named rA.B.C.D.E-hse-X.Y.Z where A.B.C is the MongoDB version (e.g., 3.4.17 ) D.E is our MongoDB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific release tag is r3.4.17.2.1-hse-1.8.0. For example $ git clone https://github.com/hse-project/hse-mongo.git $ cd hse-mongo $ git checkout rA.B.C.D.E-hse-X.Y.Z Build and install MongoDB/HSE for your platform as follows. Note For r3.4.17.2.0-hse-1.7.1 see the build instructions in the release notes . RHEL 8 $ hse-packaging/build.py --clean Install the resulting packages as described above for the RHEL 8 release packages. The build displays the package location by executing $ find /tmp/$(id -un)/pkgbuild/repo/ -name hse-mongo*.rpm Ubuntu 18.04 $ hse-packaging/build.py --clean Install the resulting packages as described above for the Ubuntu 18.04 release packages. The build displays the package location by executing $ find /tmp/$(id -un)/pkgbuild/repo/ -name hse-mongo*.deb RHEL 7 $ hse-packaging/build.py --clean Install the resulting packages as described above for the RHEL 7 release packages. The build displays the package location by executing $ find /tmp/$(id -un)/pkgbuild/repo/ -name hse-mongo*.rpm New MongoDB Options MongoDB with HSE adds the following command-line options to mongod , which are reflected in mongod --help . --hseMpoolName is the name of the mpool (KVDB) storing the MongoDB data; default is mp1 --hseParams is a string of semi-colon separated HSE parameters; e.g., \"kvdb.dur_intvl_ms=1000;kvdb.log_lvl=6\" --hseConfigPath is the path to an HSE configuration file --hseCollectionCompression is the compression algorithm applied ( lz4 or none ); default is lz4 --hseCollectionCompressionMinBytes is the min document size (bytes) to compress; default is 0 These same HSE options are also supported in mongod.conf , in addition to the standard storage configuration options. # Standard options storage: dbPath: /var/lib/mongo journal: enabled: true commitIntervalMs: 100 # Use Heterogeneous-memory Storage Engine (HSE). This is the default. # Specify the mpool (KVDB) for storing data. Default is \"mp1\". engine: hse hse: mpoolName: mongoData # Uncomment to disable or customize compression for HSE. # Allowable compression types are \"lz4\" or \"none\". Default is \"lz4\". # collectionCompression: none # collectionCompressionMinBytes: 0 # Uncomment and edit to use an HSE configuration file. # configPath: <path to HSE config file> # HSE parameters for tuning, support, or debug # params: # Recommended oplog size for HSE when using replica sets. replication: oplogSizeMB: 32000 replSetName: rs1 # Recommended query and other parameters for HSE setParameter: internalQueryExecYieldIterations: 100000 internalQueryExecYieldPeriodMS: 1000 replWriterThreadCount: 64 Options specified in hse.params or --hseParams take precedence over those specified in an HSE configuration file specified via hse.configPath or --hseConfigPath . Note Use the MongoDB hse.collectionCompression or --hseCollectionCompression option to specify value compression, not the native HSE kvs.value_compression parameter. Run MongoDB with HSE Start and manage mongod as you would normally. The owner (UID) and group owner (GID) of the mpool that mongod will use to store data should match the user account running mongod . If mongod was installed from packages, which is normally the case, you should use the mongod user account. See the release notes for any limitations or known issues for a particular release of MongoDB/HSE. YCSB Performance Results Below are results from running YCSB with MongoDB/HSE. For comparison, we include results from running YCSB with MongoDB 4.2.4 using the default WiredTiger storage engine (MongoDB/WiredTiger). Info MongoDB 4.2.4 was the latest version available at the time we conducted this performance study, which is why we selected it for comparison. We assume there have been performance improvements made since MongoDB 3.4.17, and that integrating HSE with MongoDB 4.2.4 would likely produce even better MongoDB/HSE results than those shown below. Server configuration for each member of a 3-node replica set: 2-socket Intel\u00ae Xeon\u00ae CPU E5-2690 v4 256GB DRAM 4x Micron\u00ae 9300 NVMe SSDs 3.2TB in an LVM striped logical volume RHEL 8.1 MongoDB 3.4.17 with HSE 1.7.0 (MongoDB/HSE) MongoDB 4.2.4 with WiredTiger (MongoDB/WiredTiger) YCSB benchmark configuration: 2TB dataset consisting of 2-billion 1,000-byte records 96 client threads 2 billion operations per workload YCSB 0.17.0 For MongoDB/HSE, we specified the HSE configuration file /opt/hse/config/mongodb_ycsb.yml that is installed with HSE. We also followed all recommended mongod.conf settings for HSE from the example above. The following table summarizes the YCSB workloads presented here. The application examples come from the YCSB documentation. YCSB Workload Operations Application Example A 50% Read; 50% Update Session store recording user-session activity B 95% Read; 5% Update Photo tagging C 100% Read User profile cache D 95% Read; 5% Insert User status updates Load Phase YCSB starts by populating the dataset (database) to the size specified. This is a 100% Insert workload. Load phase statistics are presented in the following table. For this workload, MongoDB/HSE delivered more than 6x the throughput compared to MongoDB/WiredTiger \u2014 reducing total duration by 84%. Load Metric MongoDB/HSE MongoDB/WiredTiger Duration (minutes) 208 1,307 Inserts / second 159,996 25,512 Insert 99.9% latency (ms) 2.7 350.7 Run Phase Run phase throughputs for MongoDB/HSE and MongoDB/WiredTiger are shown in the following chart. For these YCSB workloads, MongoDB/HSE delivered up to nearly 8x more throughput than MongoDB/WiredTiger. In delivering high throughput, MongoDB/HSE also demonstrated good 99.9% tail latency, as shown below. For these YCSB workloads, MongoDB/HSE reduced read tail latency up to 98%, and write (insert or update) tail latency by as much as 96%. Finally, we measured the amount of data written to and read from the drives on the primary cluster node in the course of executing each workload. Reducing writes is important for SSDs because it translates to increased endurance. Reducing both writes and reads is important for networked storage to reduce load on the fabric. The following chart shows the total bytes of data written to or read from the drives during workload execution. For these YCSB workloads, MongoDB/HSE reduced bytes read up to 83%, and bytes written by as much as 71%.","title":"MongoDB"},{"location":"apps/mongodb/#mongodb","text":"MongoDB\u00ae is a popular document-oriented NoSQL database. The MongoDB implementation is open source, and includes an extensible framework for integrating different storage engines. We integrated HSE with MongoDB 3.4.17 to validate its benefits within a real-world storage application. The hse-mongo repo is a fork of MongoDB that adds support for HSE. MongoDB with HSE (MongoDB/HSE) demonstrates significantly increased performance and scalability for many workloads when storing data on SSDs. In the sections that follow, the reader is assumed to be familiar with configuring and running MongoDB. The information provided here is specific to using MongoDB with HSE.","title":"MongoDB"},{"location":"apps/mongodb/#install-hse-and-create-a-kvdb","text":"Review the getting started section of this documentation. Then, following those instructions and examples, start by Installing HSE and mpool from packages or source Configuring an mpool for HSE storage with appropriate permissions Creating an HSE KVDB in that mpool to store the MongoDB data In the mongod.conf example below, we assume the name of the mpool is mongoData , which is also the name of the KVDB.","title":"Install HSE and Create a KVDB"},{"location":"apps/mongodb/#install-mongodb-dependencies","text":"Install dependencies for your platform. RHEL 8 $ sudo dnf install python2-pip gcc-toolset-9 libuuid-devel lz4 lz4-devel openssl-devel openssl-devel numactl libpcap libpcap-devel golang-1.11.13 createrepo rpmdevtools $ sudo alternatives --set python /usr/bin/python2 $ pip2 install --user scons Ubuntu 18.04 $ sudo apt-get install debhelper rpm golang libpcap-dev RHEL 7 $ sudo yum install devtoolset-9 rh-mongodb32-scons.noarch scl-utils libuuid-devel lz4 lz4-devel openssl-devel numactl libpcap libpcap-devel golang-1.9.4 python36 createrepo rpmdevtools","title":"Install MongoDB Dependencies"},{"location":"apps/mongodb/#install-mongodb-with-hse-from-packages","text":"MongoDB/HSE can be installed from release packages. Download and install the latest hse-mongo packages for your platform. Package names start with hse-mongodb-*A.B.C.D.E-X.Y.Z , where A.B.C is the MongoDB version (e.g., 3.4.17 ) D.E is our MongoDB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific prefix is hse-mongodb-server-3.4.17.2.1-1.8.0. Install the packages for your platform as follows. RHEL 8 $ sudo dnf install ./hse-mongodb-server-A.B.C.D.E-X.Y.Z*.rpm $ sudo dnf install ./hse-mongodb-mongos-A.B.C.D.E-X.Y.Z*.rpm $ sudo dnf install ./hse-mongodb-shell-A.B.C.D.E-X.Y.Z*.rpm $ sudo dnf install ./hse-mongodb-tools-A.B.C.D.E-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./hse-mongodb-server_A.B.C.D.E-X.Y.Z*.deb $ sudo apt-get install ./hse-mongodb-mongos_A.B.C.D.E-X.Y.Z*.deb $ sudo apt-get install ./hse-mongodb-shell_A.B.C.D.E-X.Y.Z*.deb $ sudo apt-get install ./hse-mongodb-tools_A.B.C.D.E-X.Y.Z*.deb RHEL 7 $ sudo yum install ./hse-mongodb-server-A.B.C.D.E-X.Y.Z*.rpm $ sudo yum install ./hse-mongodb-mongos-A.B.C.D.E-X.Y.Z*.rpm $ sudo yum install ./hse-mongodb-shell-A.B.C.D.E-X.Y.Z*.rpm $ sudo yum install ./hse-mongodb-tools-A.B.C.D.E-X.Y.Z*.rpm","title":"Install MongoDB with HSE from Packages"},{"location":"apps/mongodb/#install-mongodb-with-hse-from-source","text":"MongoDB/HSE can also be built and installed from source. Clone the latest release tag from the hse-mongo repo . Releases are named rA.B.C.D.E-hse-X.Y.Z where A.B.C is the MongoDB version (e.g., 3.4.17 ) D.E is our MongoDB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific release tag is r3.4.17.2.1-hse-1.8.0. For example $ git clone https://github.com/hse-project/hse-mongo.git $ cd hse-mongo $ git checkout rA.B.C.D.E-hse-X.Y.Z Build and install MongoDB/HSE for your platform as follows. Note For r3.4.17.2.0-hse-1.7.1 see the build instructions in the release notes . RHEL 8 $ hse-packaging/build.py --clean Install the resulting packages as described above for the RHEL 8 release packages. The build displays the package location by executing $ find /tmp/$(id -un)/pkgbuild/repo/ -name hse-mongo*.rpm Ubuntu 18.04 $ hse-packaging/build.py --clean Install the resulting packages as described above for the Ubuntu 18.04 release packages. The build displays the package location by executing $ find /tmp/$(id -un)/pkgbuild/repo/ -name hse-mongo*.deb RHEL 7 $ hse-packaging/build.py --clean Install the resulting packages as described above for the RHEL 7 release packages. The build displays the package location by executing $ find /tmp/$(id -un)/pkgbuild/repo/ -name hse-mongo*.rpm","title":"Install MongoDB with HSE from Source"},{"location":"apps/mongodb/#new-mongodb-options","text":"MongoDB with HSE adds the following command-line options to mongod , which are reflected in mongod --help . --hseMpoolName is the name of the mpool (KVDB) storing the MongoDB data; default is mp1 --hseParams is a string of semi-colon separated HSE parameters; e.g., \"kvdb.dur_intvl_ms=1000;kvdb.log_lvl=6\" --hseConfigPath is the path to an HSE configuration file --hseCollectionCompression is the compression algorithm applied ( lz4 or none ); default is lz4 --hseCollectionCompressionMinBytes is the min document size (bytes) to compress; default is 0 These same HSE options are also supported in mongod.conf , in addition to the standard storage configuration options. # Standard options storage: dbPath: /var/lib/mongo journal: enabled: true commitIntervalMs: 100 # Use Heterogeneous-memory Storage Engine (HSE). This is the default. # Specify the mpool (KVDB) for storing data. Default is \"mp1\". engine: hse hse: mpoolName: mongoData # Uncomment to disable or customize compression for HSE. # Allowable compression types are \"lz4\" or \"none\". Default is \"lz4\". # collectionCompression: none # collectionCompressionMinBytes: 0 # Uncomment and edit to use an HSE configuration file. # configPath: <path to HSE config file> # HSE parameters for tuning, support, or debug # params: # Recommended oplog size for HSE when using replica sets. replication: oplogSizeMB: 32000 replSetName: rs1 # Recommended query and other parameters for HSE setParameter: internalQueryExecYieldIterations: 100000 internalQueryExecYieldPeriodMS: 1000 replWriterThreadCount: 64 Options specified in hse.params or --hseParams take precedence over those specified in an HSE configuration file specified via hse.configPath or --hseConfigPath . Note Use the MongoDB hse.collectionCompression or --hseCollectionCompression option to specify value compression, not the native HSE kvs.value_compression parameter.","title":"New MongoDB Options"},{"location":"apps/mongodb/#run-mongodb-with-hse","text":"Start and manage mongod as you would normally. The owner (UID) and group owner (GID) of the mpool that mongod will use to store data should match the user account running mongod . If mongod was installed from packages, which is normally the case, you should use the mongod user account. See the release notes for any limitations or known issues for a particular release of MongoDB/HSE.","title":"Run MongoDB with HSE"},{"location":"apps/mongodb/#ycsb-performance-results","text":"Below are results from running YCSB with MongoDB/HSE. For comparison, we include results from running YCSB with MongoDB 4.2.4 using the default WiredTiger storage engine (MongoDB/WiredTiger). Info MongoDB 4.2.4 was the latest version available at the time we conducted this performance study, which is why we selected it for comparison. We assume there have been performance improvements made since MongoDB 3.4.17, and that integrating HSE with MongoDB 4.2.4 would likely produce even better MongoDB/HSE results than those shown below. Server configuration for each member of a 3-node replica set: 2-socket Intel\u00ae Xeon\u00ae CPU E5-2690 v4 256GB DRAM 4x Micron\u00ae 9300 NVMe SSDs 3.2TB in an LVM striped logical volume RHEL 8.1 MongoDB 3.4.17 with HSE 1.7.0 (MongoDB/HSE) MongoDB 4.2.4 with WiredTiger (MongoDB/WiredTiger) YCSB benchmark configuration: 2TB dataset consisting of 2-billion 1,000-byte records 96 client threads 2 billion operations per workload YCSB 0.17.0 For MongoDB/HSE, we specified the HSE configuration file /opt/hse/config/mongodb_ycsb.yml that is installed with HSE. We also followed all recommended mongod.conf settings for HSE from the example above. The following table summarizes the YCSB workloads presented here. The application examples come from the YCSB documentation. YCSB Workload Operations Application Example A 50% Read; 50% Update Session store recording user-session activity B 95% Read; 5% Update Photo tagging C 100% Read User profile cache D 95% Read; 5% Insert User status updates","title":"YCSB Performance Results"},{"location":"apps/mongodb/#load-phase","text":"YCSB starts by populating the dataset (database) to the size specified. This is a 100% Insert workload. Load phase statistics are presented in the following table. For this workload, MongoDB/HSE delivered more than 6x the throughput compared to MongoDB/WiredTiger \u2014 reducing total duration by 84%. Load Metric MongoDB/HSE MongoDB/WiredTiger Duration (minutes) 208 1,307 Inserts / second 159,996 25,512 Insert 99.9% latency (ms) 2.7 350.7","title":"Load Phase"},{"location":"apps/mongodb/#run-phase","text":"Run phase throughputs for MongoDB/HSE and MongoDB/WiredTiger are shown in the following chart. For these YCSB workloads, MongoDB/HSE delivered up to nearly 8x more throughput than MongoDB/WiredTiger. In delivering high throughput, MongoDB/HSE also demonstrated good 99.9% tail latency, as shown below. For these YCSB workloads, MongoDB/HSE reduced read tail latency up to 98%, and write (insert or update) tail latency by as much as 96%. Finally, we measured the amount of data written to and read from the drives on the primary cluster node in the course of executing each workload. Reducing writes is important for SSDs because it translates to increased endurance. Reducing both writes and reads is important for networked storage to reduce load on the fabric. The following chart shows the total bytes of data written to or read from the drives during workload execution. For these YCSB workloads, MongoDB/HSE reduced bytes read up to 83%, and bytes written by as much as 71%.","title":"Run Phase"},{"location":"apps/ycsb/","text":"YCSB YCSB (Yahoo!\u00ae Cloud Serving Benchmark) is an industry-standard benchmark for databases and storage engines supporting key-value workloads. The YCSB implementation is open source, and includes an extensible framework for integrating different data stores. We integrated HSE with YCSB to make it easy to compare its performance and scalability to that of other storage engines for YCSB workloads. The hse-ycsb repo is a fork of YCSB that adds support for HSE. In the sections that follow, the reader is assumed to be familiar with configuring and running the YCSB benchmark. The information provided here is specific to using YCSB with HSE. Install HSE and Create a KVDB Review the getting started section of this documentation. Then, following those instructions and examples, start by Installing HSE and mpool from packages or source Configuring an mpool for HSE storage with appropriate permissions Creating an HSE KVDB in that mpool to store the YCSB data In the examples below, we assume the name of the mpool is ycsbData , which is also the name of the KVDB. Install YCSB Dependencies Install dependencies for your platform. RHEL 8 $ sudo dnf install maven rpm-build Ubuntu 18.04 $ sudo apt-get install maven RHEL 7 $ sudo yum install rh-maven36 java-1.8.0-openjdk java-1.8.0-openjdk-devel rpm-build Install YCSB with HSE from Packages YCSB with HSE can be installed from release packages. Download and install the latest hse-ycsb package for your platform. Package names start with hse-ycsb*A.B.C.D.E-X.Y.Z , where A.B.C is the YCSB version (e.g., 0.17.0 ) D.E is our YCSB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific prefix is hse-ycsb-0.17.0.2.1-1.8.0. Install the package for your platform as follows. RHEL 8 $ sudo dnf install ./hse-ycsb-A.B.C.D.E-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./hse-ycsb_A.B.C.D.E-X.Y.Z*.deb RHEL 7 $ sudo yum install ./hse-ycsb-A.B.C.D.E-X.Y.Z*.rpm Install YCSB with HSE from Source YCSB with HSE can also be built and installed from source. Clone the latest release tag from the hse-ycsb repo . Releases are named rA.B.C.D.E-hse-X.Y.Z where A.B.C is the YCSB version (e.g., 0.17.0 ) D.E is our YCSB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific release tag is r0.17.0.2.1-hse-1.8.0. For example $ git clone https://github.com/hse-project/hse-ycsb.git $ cd hse-ycsb $ git checkout rA.B.C.D.E-hse-X.Y.Z Build and install YCSB for your platform as follows. RHEL 8 $ make package Note For r0.17.0.2.0-hse-1.7.1 instead use make rpm Install the resulting package as described above for the RHEL 8 release package. You can locate the package built by executing $ find /tmp/$(id -un)/ -name hse-ycsb*.rpm Ubuntu 18.04 $ make package Install the resulting package as described above for the Ubuntu 18.04 release package. You can locate the package built by executing $ find /tmp/$(id -un)/ -name hse-ycsb*.deb RHEL 7 $ scl enable rh-maven36 \"make rpm\" Install the resulting package as described above for the RHEL 7 release package. You can locate the package built by executing $ find /tmp/$(id -un)/ -name hse-ycsb*.rpm New YCSB Options The build of YCSB with HSE adds the following options. hse.mpool_name is the name of the mpool (KVDB) storing the YCSB data hse.params is a string of comma-separated HSE parameters; e.g., \"kvdb.dur_intvl_ms=1000,kvdb.log_lvl=6\" hse.config_path is the path to an HSE configuration file HSE also installs configuration files in /opt/hse/config for use with YCSB. There are separate configuration files for each of load and run phases. Furthermore, YCSB Workload E has its own pair of HSE configuration files. Run YCSB with HSE The following executes YCSB Workload A with HSE $ cd /opt/hse-ycsb/ $ sudo alternatives --set python /usr/bin/python2 $ ./bin/ycsb load hse -threads 96 -P workloads/workloada -p recordcount=200000 -p hse.mpool_name=ycsbData -p hse.config_path=/opt/hse/config/native_ycsb_abcdf_load.yml $ ./bin/ycsb run hse -threads 96 -P workloads/workloada -p recordcount=200000 -p operationcount=200000 -p hse.mpool_name=ycsbData -p hse.config_path=/opt/hse/config/native_ycsb_abcdf_run.yml The owner (UID) and group owner (GID) of the mpool that ycsb will use to store data should match the user account running ycsb . See the release notes for any limitations or known issues for a particular release of YCSB with HSE. YCSB Performance Results Below are results from running YCSB with HSE. For comparison, we include results from running YCSB with RocksDB , a popular and widely-deployed key-value store. Server configuration: 2-socket Intel\u00ae Xeon\u00ae CPU E5-2690 v4 256GB DRAM 4x Micron\u00ae 9300 NVMe SSDs 3.2TB in an LVM striped logical volume RHEL 8.1 HSE 1.7.0 RocksDB 6.6.4 YCSB benchmark configuration: 2TB dataset consisting of 2-billion 1,000-byte records 96 client threads 2 billion operations per workload YCSB 0.17.0 For HSE, we specified the configuration files /opt/hse/config/native_ycsb_abcdf_load.yml and /opt/hse/config/native_ycsb_abcdf_run.yml for the load and run phases of YCSB, respectively. These are installed with HSE. The following table summarizes the YCSB workloads presented here. The application examples come from the YCSB documentation. YCSB Workload Operations Application Example A 50% Read; 50% Update Session store recording user-session activity B 95% Read; 5% Update Photo tagging C 100% Read User profile cache D 95% Read; 5% Insert User status updates Load Phase YCSB starts by populating the dataset (database) to the size specified. This is a 100% Insert workload. Load phase statistics are presented in the following table. For this workload, HSE delivered more than 8x the throughput compared to RocksDB \u2014 reducing total duration by 88%. Load Metric HSE RocksDB Duration (minutes) 34 271 Inserts / second 986,009 122,899 Insert 99.9% latency (ms) 1.6 19.5 Run Phase Run phase throughputs for HSE and RocksDB are shown in the following chart. For these YCSB workloads, HSE delivered up to nearly 6x more throughput than RocksDB. In delivering high throughput, HSE also demonstrated good 99.9% tail latency, as shown below. For these YCSB workloads, HSE reduced read tail latency up to 42%, and write (insert or update) tail latency by as much as 91%. Finally, we measured the amount of data written to and read from the drives in the course of executing each workload. Reducing writes is important for SSDs because it translates to increased endurance. Reducing both writes and reads is important for networked storage to reduce load on the fabric. The following chart shows the total bytes of data written to or read from the drives during workload execution. For these YCSB workloads, HSE reduced bytes read up to 64%, and bytes written by as much as 85%.","title":"YCSB"},{"location":"apps/ycsb/#ycsb","text":"YCSB (Yahoo!\u00ae Cloud Serving Benchmark) is an industry-standard benchmark for databases and storage engines supporting key-value workloads. The YCSB implementation is open source, and includes an extensible framework for integrating different data stores. We integrated HSE with YCSB to make it easy to compare its performance and scalability to that of other storage engines for YCSB workloads. The hse-ycsb repo is a fork of YCSB that adds support for HSE. In the sections that follow, the reader is assumed to be familiar with configuring and running the YCSB benchmark. The information provided here is specific to using YCSB with HSE.","title":"YCSB"},{"location":"apps/ycsb/#install-hse-and-create-a-kvdb","text":"Review the getting started section of this documentation. Then, following those instructions and examples, start by Installing HSE and mpool from packages or source Configuring an mpool for HSE storage with appropriate permissions Creating an HSE KVDB in that mpool to store the YCSB data In the examples below, we assume the name of the mpool is ycsbData , which is also the name of the KVDB.","title":"Install HSE and Create a KVDB"},{"location":"apps/ycsb/#install-ycsb-dependencies","text":"Install dependencies for your platform. RHEL 8 $ sudo dnf install maven rpm-build Ubuntu 18.04 $ sudo apt-get install maven RHEL 7 $ sudo yum install rh-maven36 java-1.8.0-openjdk java-1.8.0-openjdk-devel rpm-build","title":"Install YCSB Dependencies"},{"location":"apps/ycsb/#install-ycsb-with-hse-from-packages","text":"YCSB with HSE can be installed from release packages. Download and install the latest hse-ycsb package for your platform. Package names start with hse-ycsb*A.B.C.D.E-X.Y.Z , where A.B.C is the YCSB version (e.g., 0.17.0 ) D.E is our YCSB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific prefix is hse-ycsb-0.17.0.2.1-1.8.0. Install the package for your platform as follows. RHEL 8 $ sudo dnf install ./hse-ycsb-A.B.C.D.E-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./hse-ycsb_A.B.C.D.E-X.Y.Z*.deb RHEL 7 $ sudo yum install ./hse-ycsb-A.B.C.D.E-X.Y.Z*.rpm","title":"Install YCSB with HSE from Packages"},{"location":"apps/ycsb/#install-ycsb-with-hse-from-source","text":"YCSB with HSE can also be built and installed from source. Clone the latest release tag from the hse-ycsb repo . Releases are named rA.B.C.D.E-hse-X.Y.Z where A.B.C is the YCSB version (e.g., 0.17.0 ) D.E is our YCSB integration version X.Y.Z is the minimum HSE release version required Tip An example of a specific release tag is r0.17.0.2.1-hse-1.8.0. For example $ git clone https://github.com/hse-project/hse-ycsb.git $ cd hse-ycsb $ git checkout rA.B.C.D.E-hse-X.Y.Z Build and install YCSB for your platform as follows. RHEL 8 $ make package Note For r0.17.0.2.0-hse-1.7.1 instead use make rpm Install the resulting package as described above for the RHEL 8 release package. You can locate the package built by executing $ find /tmp/$(id -un)/ -name hse-ycsb*.rpm Ubuntu 18.04 $ make package Install the resulting package as described above for the Ubuntu 18.04 release package. You can locate the package built by executing $ find /tmp/$(id -un)/ -name hse-ycsb*.deb RHEL 7 $ scl enable rh-maven36 \"make rpm\" Install the resulting package as described above for the RHEL 7 release package. You can locate the package built by executing $ find /tmp/$(id -un)/ -name hse-ycsb*.rpm","title":"Install YCSB with HSE from Source"},{"location":"apps/ycsb/#new-ycsb-options","text":"The build of YCSB with HSE adds the following options. hse.mpool_name is the name of the mpool (KVDB) storing the YCSB data hse.params is a string of comma-separated HSE parameters; e.g., \"kvdb.dur_intvl_ms=1000,kvdb.log_lvl=6\" hse.config_path is the path to an HSE configuration file HSE also installs configuration files in /opt/hse/config for use with YCSB. There are separate configuration files for each of load and run phases. Furthermore, YCSB Workload E has its own pair of HSE configuration files.","title":"New YCSB Options"},{"location":"apps/ycsb/#run-ycsb-with-hse","text":"The following executes YCSB Workload A with HSE $ cd /opt/hse-ycsb/ $ sudo alternatives --set python /usr/bin/python2 $ ./bin/ycsb load hse -threads 96 -P workloads/workloada -p recordcount=200000 -p hse.mpool_name=ycsbData -p hse.config_path=/opt/hse/config/native_ycsb_abcdf_load.yml $ ./bin/ycsb run hse -threads 96 -P workloads/workloada -p recordcount=200000 -p operationcount=200000 -p hse.mpool_name=ycsbData -p hse.config_path=/opt/hse/config/native_ycsb_abcdf_run.yml The owner (UID) and group owner (GID) of the mpool that ycsb will use to store data should match the user account running ycsb . See the release notes for any limitations or known issues for a particular release of YCSB with HSE.","title":"Run YCSB with HSE"},{"location":"apps/ycsb/#ycsb-performance-results","text":"Below are results from running YCSB with HSE. For comparison, we include results from running YCSB with RocksDB , a popular and widely-deployed key-value store. Server configuration: 2-socket Intel\u00ae Xeon\u00ae CPU E5-2690 v4 256GB DRAM 4x Micron\u00ae 9300 NVMe SSDs 3.2TB in an LVM striped logical volume RHEL 8.1 HSE 1.7.0 RocksDB 6.6.4 YCSB benchmark configuration: 2TB dataset consisting of 2-billion 1,000-byte records 96 client threads 2 billion operations per workload YCSB 0.17.0 For HSE, we specified the configuration files /opt/hse/config/native_ycsb_abcdf_load.yml and /opt/hse/config/native_ycsb_abcdf_run.yml for the load and run phases of YCSB, respectively. These are installed with HSE. The following table summarizes the YCSB workloads presented here. The application examples come from the YCSB documentation. YCSB Workload Operations Application Example A 50% Read; 50% Update Session store recording user-session activity B 95% Read; 5% Update Photo tagging C 100% Read User profile cache D 95% Read; 5% Insert User status updates","title":"YCSB Performance Results"},{"location":"apps/ycsb/#load-phase","text":"YCSB starts by populating the dataset (database) to the size specified. This is a 100% Insert workload. Load phase statistics are presented in the following table. For this workload, HSE delivered more than 8x the throughput compared to RocksDB \u2014 reducing total duration by 88%. Load Metric HSE RocksDB Duration (minutes) 34 271 Inserts / second 986,009 122,899 Insert 99.9% latency (ms) 1.6 19.5","title":"Load Phase"},{"location":"apps/ycsb/#run-phase","text":"Run phase throughputs for HSE and RocksDB are shown in the following chart. For these YCSB workloads, HSE delivered up to nearly 6x more throughput than RocksDB. In delivering high throughput, HSE also demonstrated good 99.9% tail latency, as shown below. For these YCSB workloads, HSE reduced read tail latency up to 42%, and write (insert or update) tail latency by as much as 91%. Finally, we measured the amount of data written to and read from the drives in the course of executing each workload. Reducing writes is important for SSDs because it translates to increased endurance. Reducing both writes and reads is important for networked storage to reduce load on the fabric. The following chart shows the total bytes of data written to or read from the drives during workload execution. For these YCSB workloads, HSE reduced bytes read up to 64%, and bytes written by as much as 85%.","title":"Run Phase"},{"location":"dev/api-reference/","text":"API Reference See the HSE API documentation in the hse.h header file. There are also examples of using the HSE API in the samples directory. Using a Custom Allocator Some users may wish to substitute custom allocators where we have used C standard library allocation functions. Below are some of the functions your custom allocator may override. Be aware that overriding these or other functions may impact performance, introduce bugs, or have other unintended consequences. malloc calloc realloc free posix_memalign aligned_alloc strdup Tip HSE has some custom use-case specific allocators within it based on mmap . These cannot be substituted.","title":"API Reference"},{"location":"dev/api-reference/#api-reference","text":"See the HSE API documentation in the hse.h header file. There are also examples of using the HSE API in the samples directory.","title":"API Reference"},{"location":"dev/api-reference/#using-a-custom-allocator","text":"Some users may wish to substitute custom allocators where we have used C standard library allocation functions. Below are some of the functions your custom allocator may override. Be aware that overriding these or other functions may impact performance, introduce bugs, or have other unintended consequences. malloc calloc realloc free posix_memalign aligned_alloc strdup Tip HSE has some custom use-case specific allocators within it based on mmap . These cannot be substituted.","title":"Using a Custom Allocator"},{"location":"dev/best-practices/","text":"Best Practices The following are best practices for developing HSE client applications. Many of these are discussed in detail in the section on HSE concepts . Keys and KVS Use a multi-segment key in the common case where related sets of KV pairs are accessed together. Choose a key prefix that groups related KV pairs when keys are sorted lexicographically, and always create the KVS storing these KV pairs with a pfx_len equal to the key prefix length. Choose a key prefix for multi-segment keys that will take on a modest number of different values over a consecutive sequence of puts. For example, in a sequence of one million put operations, ideally no more than 5% of the keys will have the same key prefix value. Use a different KVS for each collection of KV pairs requiring its own multi-segment key structure. Create index KVS to efficiently implement multiple query patterns that cannot be supported with a single multi-segment key prefix. Use an unsegmented key in the case where there is no relationship between KV pairs, and always create the KVS storing these KV pairs with a pfx_len of zero (0). Cursors and Gets Always use get operations when iteration is not required. Gets are significantly faster than cursor seeks. Where iteration is required, use cursors with KVS storing multi-segment keys, and with a filter whose length is equal to or greater than the key prefix length. Otherwise, cursor performance can be greatly reduced. Use free cursors for most applications. Transaction snapshot cursors and transaction bound cursors exist to support some specialized use cases. Transactions Use transactions when required for application correctness. Otherwise, use stand-alone operations for best performance. Transactions incur overhead to enforce snapshot isolation semantics. Application Lifecycle An HSE application's lifecycle always starts with a call to hse_kvdb_init() . This function should be called only once at the very beginning of your program. From there you can create and open KVDBs, and their associated KVSs. When your application starts to shutdown, all KVSs for every KVDB must be closed before closing the KVDB. Once all KVDBs have been closed, a single call to hse_kvdb_fini() will completely shutdown the HSE subsystem within your application. Using a signal handler to close resources can also be helpful when trying to handle unexpected shutdowns of an application.","title":"Best Practices"},{"location":"dev/best-practices/#best-practices","text":"The following are best practices for developing HSE client applications. Many of these are discussed in detail in the section on HSE concepts .","title":"Best Practices"},{"location":"dev/best-practices/#keys-and-kvs","text":"Use a multi-segment key in the common case where related sets of KV pairs are accessed together. Choose a key prefix that groups related KV pairs when keys are sorted lexicographically, and always create the KVS storing these KV pairs with a pfx_len equal to the key prefix length. Choose a key prefix for multi-segment keys that will take on a modest number of different values over a consecutive sequence of puts. For example, in a sequence of one million put operations, ideally no more than 5% of the keys will have the same key prefix value. Use a different KVS for each collection of KV pairs requiring its own multi-segment key structure. Create index KVS to efficiently implement multiple query patterns that cannot be supported with a single multi-segment key prefix. Use an unsegmented key in the case where there is no relationship between KV pairs, and always create the KVS storing these KV pairs with a pfx_len of zero (0).","title":"Keys and KVS"},{"location":"dev/best-practices/#cursors-and-gets","text":"Always use get operations when iteration is not required. Gets are significantly faster than cursor seeks. Where iteration is required, use cursors with KVS storing multi-segment keys, and with a filter whose length is equal to or greater than the key prefix length. Otherwise, cursor performance can be greatly reduced. Use free cursors for most applications. Transaction snapshot cursors and transaction bound cursors exist to support some specialized use cases.","title":"Cursors and Gets"},{"location":"dev/best-practices/#transactions","text":"Use transactions when required for application correctness. Otherwise, use stand-alone operations for best performance. Transactions incur overhead to enforce snapshot isolation semantics.","title":"Transactions"},{"location":"dev/best-practices/#application-lifecycle","text":"An HSE application's lifecycle always starts with a call to hse_kvdb_init() . This function should be called only once at the very beginning of your program. From there you can create and open KVDBs, and their associated KVSs. When your application starts to shutdown, all KVSs for every KVDB must be closed before closing the KVDB. Once all KVDBs have been closed, a single call to hse_kvdb_fini() will completely shutdown the HSE subsystem within your application. Using a signal handler to close resources can also be helpful when trying to handle unexpected shutdowns of an application.","title":"Application Lifecycle"},{"location":"dev/concepts/","text":"Concepts The following describes HSE concepts that are important to understand for developing HSE client applications and making effective use of the HSE API. KVDB and KVS HSE implements a key-value database (KVDB). An application embedding HSE may have one or more KVDB open for access at a time. A KVDB comprises one or more named key-value stores (KVS), each storing a collection of key-value (KV) pairs. A KVS is analogous to a table in a relational database. The HSE API provides the standard KV operators for managing KV pairs stored in a KVS: put, get, and delete. HSE also provides advanced operations, including transactions, cursors, and prefix deletes. Finally, HSE supports a data model enabling each KVS in a KVDB to be optimized for how the KV pairs it stores will be accessed. Data Model Understanding the HSE data model is fundamental to achieving maximum performance from HSE. Adhering to the best practices of this data model can result in significantly greater performance than might be achieved otherwise. While this data model is simple, it has proven very effective. Key Structure To describe the HSE data model, we define the following terms. key \u2014 a byte string used to uniquely identify a value for storage, retrieval, and deletion in a KVS multi-segment key \u2014 a key that is logically divided into N segments, N >= 2 , arranged to group related KV pairs when keys are sorted lexicographically unsegmented key \u2014 a key not logically divided into segments; i.e., is not a multi-segment key For multi-segment keys, we further define the following. key prefix \u2014 the first K segments, 1 <= K < N , that group related KV pairs when keys are sorted lexicographically key prefix length \u2014 the length of a key prefix in bytes KVS Configuration In the common case where related sets of KV pairs are accessed together, best performance is generally achieved by defining a multi-segment key with a key prefix such that KV pairs to be accessed together are grouped (contiguous) when keys are sorted lexicographically, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) equal to the key prefix length of the multi-segment key. In the case where there is no relationship between KV pairs, best performance is generally achieved by defining an unsegmented key, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of zero (0). Keep in mind that a KVDB may contain multiple named KVS. So in the case where there are multiple collections of KV pairs, each collection can be stored in a different KVS with a key structure, and corresponding KVS pfx_len parameter, that is appropriate for that collection. This is a powerful capability that enables HSE to optimize storage and access for all KV pairs in a KVDB without compromise. Operation Support HSE provides several advanced operations with features that directly support the HSE data model. These operations are described in detail later, but we briefly discuss how they support the data model here. Cursors are used to iterate over keys in a KVS in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys whose initial bytes match the filter. The primary use for cursors is with KVS storing multi-segment keys, where the length of the specified filter is equal to or greater than the key prefix length. Used this way, cursors provide efficient iteration over sets of related KV pairs. Prefix deletes are used to atomically remove all KV pairs in a KVS with keys whose initial bytes match a specified filter. The length of the filter must be equal to the pfx_len parameter of the KVS. The primary use for prefix deletes is with KVS storing multi-segment keys. This is a powerful capability that enables sets of related KV pairs to be deleted in a single operation without cursor iteration. Transactions are used to execute a sequence of KV operations atomically. Transactions support operating on KV pairs in one or more KVS in a KVDB. This allows storing multiple collections of KV pairs in different KVS to optimize access, without giving up the ability to operate on any of those KV pairs within the context of a single transaction. Modeling Examples Below we present examples of applying the HSE data model to a real-world problem \u2014 storing and analyzing machine-generated data. Specifically, log data captured from datacenter servers. System logs are commonly retrieved from datacenter servers on a periodic basis and stored for both real-time and historical analysis. We examine several ways this data might be modeled, depending on how it will be accessed and managed. Simple Log Storage We start with a simple HSE data model for log storage using the multi-segment key defined below, with values being individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 ts Timestamp 16 2 typeID Log type identifier In this and later examples, segment names are for convenience of presentation (they do not exist in the HSE API), and segment lengths are representative. Also, partial or complete keys may be represented as tuples using segment names. For example, (sysId) , (sysID, ts) , or (sysID, ts, typeID) . We define the key prefix to be (sysID) , yielding a key prefix length of 8 bytes. Hence, we would create a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (sysID) can be used to efficiently iterate over the log records associated with the system sysID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than a (sysID, ts) , and then iterate from there. This data model makes it easy and efficient to search the log records associated with system sysID over an arbitrary time span. However, pruning those log records, for example to retain only those from the past 30 days, requires iterating over all older records and deleting them individually. Next we will look at enhancing this data model to make log record maintenance more efficient. Per-System Epoch-based Log Storage We extend the simple HSE data model for log storage to include an epoch identifier representing a well-defined time interval. For example, an epoch might be four hours in length, with 6 epochs per day, 42 epochs per week, and so forth. We assume that a log record's timestamp can be mapped to a specific epoch through simple computation. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 epochID Epoch identifier 16 8 ts Timestamp 24 2 typeID Log type identifier We define the key prefix to be (sysID, epochID) , yielding a key prefix length of 16 bytes. Hence, we would create a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of 16 bytes. With this data model and KVS configuration, a cursor with the filter (sysID, epochID) can be used to efficiently iterate over the log records associated with the system sysID within the epoch epochID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (sysID, epochID, ts) , and then iterate from there within the epoch. With this revised data model, it is still easy and efficient to search the log records associated with system sysID over an arbitrary time span, albeit with the minor inconvenience that a new cursor must be created to cross an epoch boundary. The benefit is more efficient pruning of log records, because we can now use a single prefix delete to remove all KV pairs with a specified key prefix of (sysID, epochID) . Next we will examine a variation on this per-system epoch-based data model. All-Systems Epoch-based Log Storage The multi-segment key and key prefix defined in the previous HSE data model make it easy and efficient to iterate over the log records associated with a given sysID for a specified epochID . However, to view records from multiple systems for a specified epochID requires a cursor iteration per system. We can instead group the log records for all systems within an epoch using the following multi-segment key. Key offset Segment Length (bytes) Segment Name Description 0 8 epochID Epoch identifier 8 8 ts Timestamp 16 8 sysID System identifier 24 2 typeID Log type identifier We define the key prefix to be (epochID) , yielding a key prefix length of 8 bytes. Hence we would create a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (epochID) can be used to efficiently iterate over the log records for all systems within the epoch epochID in timestamp order. Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (epochID, ts) , and then iterate from there within the epoch. It should be clear that there is some loss of efficiency, versus the prior data model, to obtain all the log records associated with a specified sysID over an arbitrary time span, because records associated with systems other than sysID will be in the cursor's view and must be skipped. However, this is still a reasonably efficient query because it meets the critical criteria that the length of the cursor filter be equal to or greater than the key prefix length to achieve maximum performance. With this revised data model, pruning log records remains efficient because we can still use a single prefix delete to remove all KV pairs with a specified key prefix of (epochID) . In this case, the records for all systems in the epoch are pruned together. Tip This particular data model provides the opportunity to point out another best practice that can result in significantly greater performance. For a KVS storing multi-segment keys, it is important that the key prefix specified in put operations takes on a modest number of different values over a consecutive sequence of puts. For this example, that means choosing an epoch that is relatively short versus what one might select with the prior data model. Finally, we will examine an index-based data model for log storage. Index-based Log Storage The prior HSE data models for log storage have the benefit of simplicity. However, they may provide less flexibility than required by the client application. In this example, we demonstrate how to use multiple KVS to effectively index log records. For brevity, we define the key structure for each KVS using the tuple syntax, segment names, and segment lengths adopted in prior examples. The only new element is logID , which uniquely identifies a log record. KVS Name Key Type Key Key Prefix KVS pfx_len Value logRec unsegmented (logID) 0 Log record content sysIdx multi-segment (sysID, epochID, ts, typeID, logID) (sysID, epochID) 16 epochIdx multi-segment (epochID, ts, sysID, typeID, logID) (epochID) 8 The KVS logRec stores the content of all log records, with each log record identified by a unique unsegmented key logID . Because KVS logRec stores KV pairs with an unsegmented key, it is configured with a key prefix length parameter ( pfx_len ) of 0 bytes. The two KVS sysIdx and epochIdx are indexes for the log records stored in KVS logRec . These KVS are nearly identical to those created in the prior data model examples for per-system epoch-based log storage and all-systems epoch-based log storage, respectively. The only differences are that logID is added to the multi-segment keys as the final segment, and their are no associated values (i.e., the values are zero length). The same cursor queries apply to sysIdx and epochIdx as in those prior data models, with the added step that to view the log content requires a get operation with key logID from the KVS logRec . The benefit is that we can use sysIdx to efficiently search the log records for a given sysID within any time span, or we can use epochIdx to efficiently search the log records for all systems within that time span. A transaction can be used to atomically put (insert) the KV pairs for a given log record in all of logRec , sysIdx , and epochIdx . This ensures integrity of the indexes. Pruning log records is more complex than the prior data models. One approach to deleting all log records for a given epochID , where the epoch is assumed to have passed, is to use a cursor with filter (epochID) to iterate over KVS epochIdx to build lists of all sysID and logID in the epoch; and then in a transaction delete all logID from logRec ; prefix delete each key prefix (sysID, epochID) from sysIdx ; and prefix delete the key prefix (epochID) from epochIdx . Snapshots HSE implements industry-standard snapshot isolation semantics. The implication is that transactions and cursors operate on KVS snapshots. Conceptually, a KVS snapshot contains KV pairs from all transactions committed, and stand-alone operations completed, at the time the snapshot is taken. In this context, a stand-alone KV operation is one performed outside of a transaction. A KVS snapshot is ephemeral and ceases to exist when all associated operations complete. Transactions Transactions may be used to execute a sequence of KV operations as a unit of work that is atomic, consistent, isolated, and durable (ACID). A transaction may operate on KV pairs in one or more KVS in a KVDB. Conceptually, when a transaction is initiated an instantaneous snapshot is taken of all KVS in the specified KVDB. The transaction may then be used to read or update KV pairs in these KVS snapshots. Snapshot isolation is enforced by failing updates (puts or deletes) that collide with updates in concurrent transactions, after which the transaction may be aborted and retried. In rare cases, the collision detection mechanism may produce false positives. Stand-alone KV operations are not visible to, and do not impact, concurrent transactions. For example, a stand-alone put operation is not visible to, and will not collide with, any concurrent transaction. HSE implements asynchronous (non-durable) transaction commits. Committed transactions are made durable either via explicit HSE API calls, or automatically within the durability interval ( dur_intvl_ms ) configured for a KVDB. HSE implements transactions using multiversion concurrency control (MVCC) techniques supporting a high-degree of transaction concurrency. Cursors Cursors are used to iterate over keys in a KVS snapshot. A cursor can iterate over keys in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys in a KVS snapshot whose initial bytes match the filter. Tip Cursors deliver significantly greater performance when used with KVS storing multi-segment keys, and where a filter is specified with a length equal to or greater than the key prefix length. To the degree practical, you should structure applications to avoid using cursors outside of this use case. Furthermore, you should always use get operations instead of cursor seeks when iteration is not required. A cursor can be used to seek to the first key in the cursor's view that is lexicographically equal to or greater than a specified key. The interaction of cursor filters and seek is best described by example. Consider a KVS storing the following keys, which are listed in lexicographic order: \"ab001\", \"af001\", \"af002\", \"ap001\". If a cursor is created for the KVS with a filter of \"af\", then the cursor's view is limited to the keys: \"af001\", \"af002\". If that cursor is then used to seek to the key \"ab\", it will be positioned at the first key in its view equal to or greater than \"ab\", which is \"af001\". Iterating (reading) with the cursor will return the key \"af001\", then \"af002\", and then the EOF condition indicating there are no more keys in view. If instead the cursor is used to seek to the key \"ap\", it will be positioned past the last key in its view, such that an attempt to iterate (read) with the cursor will indicate an EOF condition. There are several different types of cursors. A free cursor iterates over a KVS snapshot that is taken at the time the cursor is created. The view of a free cursor can be explicitly updated to the latest snapshot of the KVS. Most applications will use a free cursor for iteration. A transaction snapshot cursor iterates over a KVS snapshot associated with an active transaction. The cursor does not have in its view any updates made in that transaction, and its view does not change when the transaction commits or aborts. A transaction bound cursor iterates over a KVS snapshot associated with an active transaction, including any updates made in that transaction. A transaction bound cursor becomes a free cursor when that transaction commits or aborts. The view of the (now free) cursor depends on if the static view flag was specified when the transaction bound cursor was created. With static view flag: the cursor continues with the same KVS snapshot, meaning any updates made in the transaction are no longer in its view. Without static view flag and transaction commits : the cursor's view is updated to the KVS snapshot immediately following the transaction commit, which will include updates made in that transaction, and potentially in other committed transactions or completed stand-alone operations. Without static view flag and transaction aborts : the cursor's view is updated to the latest snapshot of the KVS. Durability Controls HSE provides APIs for flushing cached KVDB updates to stable storage, either synchronously or asynchronously. Flushing applies to all cached updates, from both stand-alone KV operations and committed transactions. Cached updates are automatically flushed to stable storage within the durability interval ( dur_intvl_ms ) configured for a KVDB. Multithreading HSE supports highly-concurrent multithreaded applications, and most functions in the HSE API are thread-safe. However, there are a few exceptions, as documented in the HSE API reference . Delete Semantics Delete operations logically remove KV pairs from a KVS. However, HSE implements physical removal as a background operation, and hence capacity is not freed immediately.","title":"Concepts"},{"location":"dev/concepts/#concepts","text":"The following describes HSE concepts that are important to understand for developing HSE client applications and making effective use of the HSE API.","title":"Concepts"},{"location":"dev/concepts/#kvdb-and-kvs","text":"HSE implements a key-value database (KVDB). An application embedding HSE may have one or more KVDB open for access at a time. A KVDB comprises one or more named key-value stores (KVS), each storing a collection of key-value (KV) pairs. A KVS is analogous to a table in a relational database. The HSE API provides the standard KV operators for managing KV pairs stored in a KVS: put, get, and delete. HSE also provides advanced operations, including transactions, cursors, and prefix deletes. Finally, HSE supports a data model enabling each KVS in a KVDB to be optimized for how the KV pairs it stores will be accessed.","title":"KVDB and KVS"},{"location":"dev/concepts/#data-model","text":"Understanding the HSE data model is fundamental to achieving maximum performance from HSE. Adhering to the best practices of this data model can result in significantly greater performance than might be achieved otherwise. While this data model is simple, it has proven very effective.","title":"Data Model"},{"location":"dev/concepts/#key-structure","text":"To describe the HSE data model, we define the following terms. key \u2014 a byte string used to uniquely identify a value for storage, retrieval, and deletion in a KVS multi-segment key \u2014 a key that is logically divided into N segments, N >= 2 , arranged to group related KV pairs when keys are sorted lexicographically unsegmented key \u2014 a key not logically divided into segments; i.e., is not a multi-segment key For multi-segment keys, we further define the following. key prefix \u2014 the first K segments, 1 <= K < N , that group related KV pairs when keys are sorted lexicographically key prefix length \u2014 the length of a key prefix in bytes","title":"Key Structure"},{"location":"dev/concepts/#kvs-configuration","text":"In the common case where related sets of KV pairs are accessed together, best performance is generally achieved by defining a multi-segment key with a key prefix such that KV pairs to be accessed together are grouped (contiguous) when keys are sorted lexicographically, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) equal to the key prefix length of the multi-segment key. In the case where there is no relationship between KV pairs, best performance is generally achieved by defining an unsegmented key, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of zero (0). Keep in mind that a KVDB may contain multiple named KVS. So in the case where there are multiple collections of KV pairs, each collection can be stored in a different KVS with a key structure, and corresponding KVS pfx_len parameter, that is appropriate for that collection. This is a powerful capability that enables HSE to optimize storage and access for all KV pairs in a KVDB without compromise.","title":"KVS Configuration"},{"location":"dev/concepts/#operation-support","text":"HSE provides several advanced operations with features that directly support the HSE data model. These operations are described in detail later, but we briefly discuss how they support the data model here. Cursors are used to iterate over keys in a KVS in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys whose initial bytes match the filter. The primary use for cursors is with KVS storing multi-segment keys, where the length of the specified filter is equal to or greater than the key prefix length. Used this way, cursors provide efficient iteration over sets of related KV pairs. Prefix deletes are used to atomically remove all KV pairs in a KVS with keys whose initial bytes match a specified filter. The length of the filter must be equal to the pfx_len parameter of the KVS. The primary use for prefix deletes is with KVS storing multi-segment keys. This is a powerful capability that enables sets of related KV pairs to be deleted in a single operation without cursor iteration. Transactions are used to execute a sequence of KV operations atomically. Transactions support operating on KV pairs in one or more KVS in a KVDB. This allows storing multiple collections of KV pairs in different KVS to optimize access, without giving up the ability to operate on any of those KV pairs within the context of a single transaction.","title":"Operation Support"},{"location":"dev/concepts/#modeling-examples","text":"Below we present examples of applying the HSE data model to a real-world problem \u2014 storing and analyzing machine-generated data. Specifically, log data captured from datacenter servers. System logs are commonly retrieved from datacenter servers on a periodic basis and stored for both real-time and historical analysis. We examine several ways this data might be modeled, depending on how it will be accessed and managed.","title":"Modeling Examples"},{"location":"dev/concepts/#simple-log-storage","text":"We start with a simple HSE data model for log storage using the multi-segment key defined below, with values being individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 ts Timestamp 16 2 typeID Log type identifier In this and later examples, segment names are for convenience of presentation (they do not exist in the HSE API), and segment lengths are representative. Also, partial or complete keys may be represented as tuples using segment names. For example, (sysId) , (sysID, ts) , or (sysID, ts, typeID) . We define the key prefix to be (sysID) , yielding a key prefix length of 8 bytes. Hence, we would create a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (sysID) can be used to efficiently iterate over the log records associated with the system sysID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than a (sysID, ts) , and then iterate from there. This data model makes it easy and efficient to search the log records associated with system sysID over an arbitrary time span. However, pruning those log records, for example to retain only those from the past 30 days, requires iterating over all older records and deleting them individually. Next we will look at enhancing this data model to make log record maintenance more efficient.","title":"Simple Log Storage"},{"location":"dev/concepts/#per-system-epoch-based-log-storage","text":"We extend the simple HSE data model for log storage to include an epoch identifier representing a well-defined time interval. For example, an epoch might be four hours in length, with 6 epochs per day, 42 epochs per week, and so forth. We assume that a log record's timestamp can be mapped to a specific epoch through simple computation. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 epochID Epoch identifier 16 8 ts Timestamp 24 2 typeID Log type identifier We define the key prefix to be (sysID, epochID) , yielding a key prefix length of 16 bytes. Hence, we would create a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of 16 bytes. With this data model and KVS configuration, a cursor with the filter (sysID, epochID) can be used to efficiently iterate over the log records associated with the system sysID within the epoch epochID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (sysID, epochID, ts) , and then iterate from there within the epoch. With this revised data model, it is still easy and efficient to search the log records associated with system sysID over an arbitrary time span, albeit with the minor inconvenience that a new cursor must be created to cross an epoch boundary. The benefit is more efficient pruning of log records, because we can now use a single prefix delete to remove all KV pairs with a specified key prefix of (sysID, epochID) . Next we will examine a variation on this per-system epoch-based data model.","title":"Per-System Epoch-based Log Storage"},{"location":"dev/concepts/#all-systems-epoch-based-log-storage","text":"The multi-segment key and key prefix defined in the previous HSE data model make it easy and efficient to iterate over the log records associated with a given sysID for a specified epochID . However, to view records from multiple systems for a specified epochID requires a cursor iteration per system. We can instead group the log records for all systems within an epoch using the following multi-segment key. Key offset Segment Length (bytes) Segment Name Description 0 8 epochID Epoch identifier 8 8 ts Timestamp 16 8 sysID System identifier 24 2 typeID Log type identifier We define the key prefix to be (epochID) , yielding a key prefix length of 8 bytes. Hence we would create a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (epochID) can be used to efficiently iterate over the log records for all systems within the epoch epochID in timestamp order. Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (epochID, ts) , and then iterate from there within the epoch. It should be clear that there is some loss of efficiency, versus the prior data model, to obtain all the log records associated with a specified sysID over an arbitrary time span, because records associated with systems other than sysID will be in the cursor's view and must be skipped. However, this is still a reasonably efficient query because it meets the critical criteria that the length of the cursor filter be equal to or greater than the key prefix length to achieve maximum performance. With this revised data model, pruning log records remains efficient because we can still use a single prefix delete to remove all KV pairs with a specified key prefix of (epochID) . In this case, the records for all systems in the epoch are pruned together. Tip This particular data model provides the opportunity to point out another best practice that can result in significantly greater performance. For a KVS storing multi-segment keys, it is important that the key prefix specified in put operations takes on a modest number of different values over a consecutive sequence of puts. For this example, that means choosing an epoch that is relatively short versus what one might select with the prior data model. Finally, we will examine an index-based data model for log storage.","title":"All-Systems Epoch-based Log Storage"},{"location":"dev/concepts/#index-based-log-storage","text":"The prior HSE data models for log storage have the benefit of simplicity. However, they may provide less flexibility than required by the client application. In this example, we demonstrate how to use multiple KVS to effectively index log records. For brevity, we define the key structure for each KVS using the tuple syntax, segment names, and segment lengths adopted in prior examples. The only new element is logID , which uniquely identifies a log record. KVS Name Key Type Key Key Prefix KVS pfx_len Value logRec unsegmented (logID) 0 Log record content sysIdx multi-segment (sysID, epochID, ts, typeID, logID) (sysID, epochID) 16 epochIdx multi-segment (epochID, ts, sysID, typeID, logID) (epochID) 8 The KVS logRec stores the content of all log records, with each log record identified by a unique unsegmented key logID . Because KVS logRec stores KV pairs with an unsegmented key, it is configured with a key prefix length parameter ( pfx_len ) of 0 bytes. The two KVS sysIdx and epochIdx are indexes for the log records stored in KVS logRec . These KVS are nearly identical to those created in the prior data model examples for per-system epoch-based log storage and all-systems epoch-based log storage, respectively. The only differences are that logID is added to the multi-segment keys as the final segment, and their are no associated values (i.e., the values are zero length). The same cursor queries apply to sysIdx and epochIdx as in those prior data models, with the added step that to view the log content requires a get operation with key logID from the KVS logRec . The benefit is that we can use sysIdx to efficiently search the log records for a given sysID within any time span, or we can use epochIdx to efficiently search the log records for all systems within that time span. A transaction can be used to atomically put (insert) the KV pairs for a given log record in all of logRec , sysIdx , and epochIdx . This ensures integrity of the indexes. Pruning log records is more complex than the prior data models. One approach to deleting all log records for a given epochID , where the epoch is assumed to have passed, is to use a cursor with filter (epochID) to iterate over KVS epochIdx to build lists of all sysID and logID in the epoch; and then in a transaction delete all logID from logRec ; prefix delete each key prefix (sysID, epochID) from sysIdx ; and prefix delete the key prefix (epochID) from epochIdx .","title":"Index-based Log Storage"},{"location":"dev/concepts/#snapshots","text":"HSE implements industry-standard snapshot isolation semantics. The implication is that transactions and cursors operate on KVS snapshots. Conceptually, a KVS snapshot contains KV pairs from all transactions committed, and stand-alone operations completed, at the time the snapshot is taken. In this context, a stand-alone KV operation is one performed outside of a transaction. A KVS snapshot is ephemeral and ceases to exist when all associated operations complete.","title":"Snapshots"},{"location":"dev/concepts/#transactions","text":"Transactions may be used to execute a sequence of KV operations as a unit of work that is atomic, consistent, isolated, and durable (ACID). A transaction may operate on KV pairs in one or more KVS in a KVDB. Conceptually, when a transaction is initiated an instantaneous snapshot is taken of all KVS in the specified KVDB. The transaction may then be used to read or update KV pairs in these KVS snapshots. Snapshot isolation is enforced by failing updates (puts or deletes) that collide with updates in concurrent transactions, after which the transaction may be aborted and retried. In rare cases, the collision detection mechanism may produce false positives. Stand-alone KV operations are not visible to, and do not impact, concurrent transactions. For example, a stand-alone put operation is not visible to, and will not collide with, any concurrent transaction. HSE implements asynchronous (non-durable) transaction commits. Committed transactions are made durable either via explicit HSE API calls, or automatically within the durability interval ( dur_intvl_ms ) configured for a KVDB. HSE implements transactions using multiversion concurrency control (MVCC) techniques supporting a high-degree of transaction concurrency.","title":"Transactions"},{"location":"dev/concepts/#cursors","text":"Cursors are used to iterate over keys in a KVS snapshot. A cursor can iterate over keys in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys in a KVS snapshot whose initial bytes match the filter. Tip Cursors deliver significantly greater performance when used with KVS storing multi-segment keys, and where a filter is specified with a length equal to or greater than the key prefix length. To the degree practical, you should structure applications to avoid using cursors outside of this use case. Furthermore, you should always use get operations instead of cursor seeks when iteration is not required. A cursor can be used to seek to the first key in the cursor's view that is lexicographically equal to or greater than a specified key. The interaction of cursor filters and seek is best described by example. Consider a KVS storing the following keys, which are listed in lexicographic order: \"ab001\", \"af001\", \"af002\", \"ap001\". If a cursor is created for the KVS with a filter of \"af\", then the cursor's view is limited to the keys: \"af001\", \"af002\". If that cursor is then used to seek to the key \"ab\", it will be positioned at the first key in its view equal to or greater than \"ab\", which is \"af001\". Iterating (reading) with the cursor will return the key \"af001\", then \"af002\", and then the EOF condition indicating there are no more keys in view. If instead the cursor is used to seek to the key \"ap\", it will be positioned past the last key in its view, such that an attempt to iterate (read) with the cursor will indicate an EOF condition. There are several different types of cursors. A free cursor iterates over a KVS snapshot that is taken at the time the cursor is created. The view of a free cursor can be explicitly updated to the latest snapshot of the KVS. Most applications will use a free cursor for iteration. A transaction snapshot cursor iterates over a KVS snapshot associated with an active transaction. The cursor does not have in its view any updates made in that transaction, and its view does not change when the transaction commits or aborts. A transaction bound cursor iterates over a KVS snapshot associated with an active transaction, including any updates made in that transaction. A transaction bound cursor becomes a free cursor when that transaction commits or aborts. The view of the (now free) cursor depends on if the static view flag was specified when the transaction bound cursor was created. With static view flag: the cursor continues with the same KVS snapshot, meaning any updates made in the transaction are no longer in its view. Without static view flag and transaction commits : the cursor's view is updated to the KVS snapshot immediately following the transaction commit, which will include updates made in that transaction, and potentially in other committed transactions or completed stand-alone operations. Without static view flag and transaction aborts : the cursor's view is updated to the latest snapshot of the KVS.","title":"Cursors"},{"location":"dev/concepts/#durability-controls","text":"HSE provides APIs for flushing cached KVDB updates to stable storage, either synchronously or asynchronously. Flushing applies to all cached updates, from both stand-alone KV operations and committed transactions. Cached updates are automatically flushed to stable storage within the durability interval ( dur_intvl_ms ) configured for a KVDB.","title":"Durability Controls"},{"location":"dev/concepts/#multithreading","text":"HSE supports highly-concurrent multithreaded applications, and most functions in the HSE API are thread-safe. However, there are a few exceptions, as documented in the HSE API reference .","title":"Multithreading"},{"location":"dev/concepts/#delete-semantics","text":"Delete operations logically remove KV pairs from a KVS. However, HSE implements physical removal as a background operation, and hence capacity is not freed immediately.","title":"Delete Semantics"},{"location":"dev/limits/","text":"Limits The tables below provide guidance on HSE operating limits. A few are enforced, but most are based on testing and experience. The limits appropriate for a specific HSE client application are largely dependent on the performance requirements of that application, and the hardware it runs on. Feel free to push these limits in a test environment, and let us know how far you get and what you observe. KVDB Limits Entity Description Limit Enforced KVDB count Active KVDB per system 8 No KVS count KVS in a KVDB 16 No Key count Total keys in a KVDB (billions) 200 No Capacity Total storage capacity of a KVDB (TB) 12 No Transaction count Concurrent transactions in a KVDB 1,000 per CPU Yes Cursor count Concurrent cursors in a KVDB 10,000 No Tip An active cursor can consume up to 2MB of memory. KVS Limits Entity Description Limit Enforced Key size Range of valid key sizes (bytes) 1 \u2013 1,334 Yes Value size Range of valid value sizes (bytes) 0 \u2013 1MiB Yes Key count Total keys in a KVS (billions) 50 No Capacity Total storage capacity of a KVS (TB) 4 No Cursor count Concurrent cursors in a KVS 8,000 No Mpool Limits These mpool limits apply only to HSE as an mpool client application. Other mpool client applications may have vastly different mpool limits. Entity Description Limit Enforced Media class capacity Volume size storing a media class (TB) 8 No Mpool capacity Combined capacity of all media classes (TB) 16 No","title":"Limits"},{"location":"dev/limits/#limits","text":"The tables below provide guidance on HSE operating limits. A few are enforced, but most are based on testing and experience. The limits appropriate for a specific HSE client application are largely dependent on the performance requirements of that application, and the hardware it runs on. Feel free to push these limits in a test environment, and let us know how far you get and what you observe.","title":"Limits"},{"location":"dev/limits/#kvdb-limits","text":"Entity Description Limit Enforced KVDB count Active KVDB per system 8 No KVS count KVS in a KVDB 16 No Key count Total keys in a KVDB (billions) 200 No Capacity Total storage capacity of a KVDB (TB) 12 No Transaction count Concurrent transactions in a KVDB 1,000 per CPU Yes Cursor count Concurrent cursors in a KVDB 10,000 No Tip An active cursor can consume up to 2MB of memory.","title":"KVDB Limits"},{"location":"dev/limits/#kvs-limits","text":"Entity Description Limit Enforced Key size Range of valid key sizes (bytes) 1 \u2013 1,334 Yes Value size Range of valid value sizes (bytes) 0 \u2013 1MiB Yes Key count Total keys in a KVS (billions) 50 No Capacity Total storage capacity of a KVS (TB) 4 No Cursor count Concurrent cursors in a KVS 8,000 No","title":"KVS Limits"},{"location":"dev/limits/#mpool-limits","text":"These mpool limits apply only to HSE as an mpool client application. Other mpool client applications may have vastly different mpool limits. Entity Description Limit Enforced Media class capacity Volume size storing a media class (TB) 8 No Mpool capacity Combined capacity of all media classes (TB) 16 No","title":"Mpool Limits"},{"location":"gs/config-storage/","text":"Configure Storage HSE uses the mpool kernel module to store data. Mpool implements an object storage device interface on SSD volumes. The term volume is used generically here to refer to a Physical drive or drive partition Logical drive, such as a Linux LVM volume, a SAN array volume, or a cloud storage volume We recommend using logical volumes when configuring mpools for HSE storage. Logical volumes provide greater flexibility in managing capacity and performance, among other benefits. Storage Configuration Example The following example demonstrates configuring an mpool to store an HSE key-value database (KVDB). The mpool is configured with a single media class (class of solid-state storage), which is the common case. In this example, Linux LVM is used to create a logical volume striped on two physical SSDs. This volume is then used to configure the required capacity media class for the mpool. Configure a logical volume Configure a striped logical volume from a volume group comprising two physical SSDs. $ sudo vgcreate vg_nvmeSSD /dev/nvme1n1 /dev/nvme2n1 $ sudo lvcreate -L 1TB -i 2 -I 64 -n mydb_capacity vg_nvmeSSD Configure an mpool Configure an mpool using the mpool CLI. This creates an mpool device file /dev/mpool/<mpool name> . Mpool uses the standard Linux security model whereby each mpool (device file) has an owner (UID), group owner (GID), and mode bits controlling access. In this example, jdoe is used for the UID and GID. We also specify a media block (mblock) size for the capacity media class of 32MB, which is recommended for all HSE media class devices. $ sudo mpool create mydb /dev/vg_nvmeSSD/mydb_capacity uid=jdoe gid=jdoe mode=0600 capsz=32 View the mpool You can now view information about the mpool using the mpool CLI, and see it in the device file namespace. $ sudo mpool list MPOOL TOTAL USED AVAIL CAPACITY LABEL HEALTH mydb 1.00t 1.16g 972g 0.12% raw optimal $ ls -l /dev/mpool crw------- 1 jdoe jdoe 238, 1 Mar 18 18:44 mydb Summary The mpool mydb can now be used to create a KVDB . Best Practices See the mpool Wiki for complete information on configuring and managing mpool storage, including increasing mpool capacity, and configuring multiple media classes. The mpool CLI also contains embedded help. Best practices for configuring an mpool for HSE storage include the following. Use SSD-backed volumes. HSE is designed and optimized for SSD performance characteristics. Configure volumes with a 4KB logical block size whenever possible. When using Linux LVM to configure logical volumes, specify a stripesize of 64KB for striped volumes. Configure mpools with a 32MB media block (mblock) size, which is the default. Configure mpools with 5% spare capacity in each media class, which is the default. Configure mpools with 40% more capacity than required to store key-value data to accommodate space amplification from HSE metadata and deferred deletes. Keep in mind that mpool capacity can be increased, though not while the stored KVDB is active (in use).","title":"Configure Storage"},{"location":"gs/config-storage/#configure-storage","text":"HSE uses the mpool kernel module to store data. Mpool implements an object storage device interface on SSD volumes. The term volume is used generically here to refer to a Physical drive or drive partition Logical drive, such as a Linux LVM volume, a SAN array volume, or a cloud storage volume We recommend using logical volumes when configuring mpools for HSE storage. Logical volumes provide greater flexibility in managing capacity and performance, among other benefits.","title":"Configure Storage"},{"location":"gs/config-storage/#storage-configuration-example","text":"The following example demonstrates configuring an mpool to store an HSE key-value database (KVDB). The mpool is configured with a single media class (class of solid-state storage), which is the common case. In this example, Linux LVM is used to create a logical volume striped on two physical SSDs. This volume is then used to configure the required capacity media class for the mpool.","title":"Storage Configuration Example"},{"location":"gs/config-storage/#configure-a-logical-volume","text":"Configure a striped logical volume from a volume group comprising two physical SSDs. $ sudo vgcreate vg_nvmeSSD /dev/nvme1n1 /dev/nvme2n1 $ sudo lvcreate -L 1TB -i 2 -I 64 -n mydb_capacity vg_nvmeSSD","title":"Configure a logical volume"},{"location":"gs/config-storage/#configure-an-mpool","text":"Configure an mpool using the mpool CLI. This creates an mpool device file /dev/mpool/<mpool name> . Mpool uses the standard Linux security model whereby each mpool (device file) has an owner (UID), group owner (GID), and mode bits controlling access. In this example, jdoe is used for the UID and GID. We also specify a media block (mblock) size for the capacity media class of 32MB, which is recommended for all HSE media class devices. $ sudo mpool create mydb /dev/vg_nvmeSSD/mydb_capacity uid=jdoe gid=jdoe mode=0600 capsz=32","title":"Configure an mpool"},{"location":"gs/config-storage/#view-the-mpool","text":"You can now view information about the mpool using the mpool CLI, and see it in the device file namespace. $ sudo mpool list MPOOL TOTAL USED AVAIL CAPACITY LABEL HEALTH mydb 1.00t 1.16g 972g 0.12% raw optimal $ ls -l /dev/mpool crw------- 1 jdoe jdoe 238, 1 Mar 18 18:44 mydb","title":"View the mpool"},{"location":"gs/config-storage/#summary","text":"The mpool mydb can now be used to create a KVDB .","title":"Summary"},{"location":"gs/config-storage/#best-practices","text":"See the mpool Wiki for complete information on configuring and managing mpool storage, including increasing mpool capacity, and configuring multiple media classes. The mpool CLI also contains embedded help. Best practices for configuring an mpool for HSE storage include the following. Use SSD-backed volumes. HSE is designed and optimized for SSD performance characteristics. Configure volumes with a 4KB logical block size whenever possible. When using Linux LVM to configure logical volumes, specify a stripesize of 64KB for striped volumes. Configure mpools with a 32MB media block (mblock) size, which is the default. Configure mpools with 5% spare capacity in each media class, which is the default. Configure mpools with 40% more capacity than required to store key-value data to accommodate space amplification from HSE metadata and deferred deletes. Keep in mind that mpool capacity can be increased, though not while the stored KVDB is active (in use).","title":"Best Practices"},{"location":"gs/create-kvdb/","text":"Create a KVDB After configuring an mpool for HSE storage, you can create (store) a KVDB in the mpool using the hse CLI. Continuing with the prior example , we create a KVDB in the mpool mydb . $ hse kvdb create mydb $ hse kvdb list kvdbs: - name: mydb label: raw The name and label of the KVDB are simply the name and label of the mpool that stores it. In a later example, we will update the label to something more descriptive. At this point, the KVDB is ready to use with one of the applications that has already been integrated with HSE, or with your own application. The remainder of this section defines HSE configuration parameters, and provides further hse CLI usage examples. For applications we have integrated with HSE, detailed configuration information is provided in their individual sections of this documentation. Configuration Parameters HSE configuration parameters can be specified in several ways. Directly in hse CLI commands as <param>=<value> pairs From a configuration file passed to the hse CLI with the -c option In HSE API calls using functions for specifying parameters directly or reading them from a configuration file The most common (and preferred) method is for an application to use HSE API calls to read parameter values from a configuration file, and then to augment or override those values as appropriate for that application. HSE parameters apply to either a KVDB, or to a key-value store (KVS) within a KVDB. A KVS in a KVDB is analogous to a table in a relational database in that each KVS stores an independent collection of key-value (KV) pairs. These and other HSE concepts are described in the section on developing HSE applications. When using the hse CLI, only persistent parameters need to be specified. Specifying non-persistent parameters has no effect. The KVDB and KVS parameter tables below specify the HSE version at which a parameter was introduced. If the version column is blank, the parameter has been available since the initial release. Note The configuration parameters listed below are part of the stable HSE API. Configuration files installed for applications that have been integrated with HSE may use experimental parameters. KVDB Parameters Parameter Description Default Persistent Version dur_intvl_ms Max time data is cached before flushing to media (ms) 500 No log_lvl Syslog severity level (0=emergency; 7=debug) 7 No read_only Read-only mode enabled (0=disabled; 1=enabled) 0 No throttle_init_policy Ingest throttle at startup (light; medium; default) default No 1.8.0 Below are additional details on setting several of these KVDB parameters. Durability Interval (dur_intvl_ms) The durability interval for a KVDB is the frequency at which cached updates from committed transactions and stand-alone KV operations are flushed to stable storage. This interval is specified by the dur_intvl_ms parameter in units of milliseconds. The concepts section provides additional discussion on HSE durability controls . Initial Throttle (throttle_init_policy) On startup, a KVDB throttles the rate at which it processes puts and deletes of KV pairs, referred to as the ingest rate. KVDB increases the ingest rate until it reaches the maximum sustainable value for the underlying mpool storage. This ramp-up process can take on the order of 50 to 200 seconds . For benchmarks, this initial throttling can greatly distort results. In production environments, this initial throttling may impact the time before a service is fully operational. The throttle_init_policy parameter can be used to achieve the maximum ingest rate in far less time. It specifies a relative initial throttling value of light , medium , or default (maximum) throttling. Setting the throttle_init_policy parameter improperly for the underlying mpool storage can cause the durability interval ( dur_intvl_ms ) to be violated, or internal indexing structures to become unbalanced until KVDB determines the maximum sustainable ingest rate for the mpool. The mpool_profile tool is provided to determine an appropriate throttle_init_policy setting for an mpool as follows $ /opt/hse/bin/mpool_profile -v mydb Use the output of mpool_profile to specify the throttle_init_policy value for a KVDB. If the mpool configuration changes, for example a staging media class is added, rerun mpool_profile as the throttle_init_policy value may change. Tip mpool_profile may run for several minutes before producing output. KVS Parameters Parameter Description Default Persistent Version pfx_len Key prefix length (bytes) 0 Yes value_compression Value compression method (lz4; none) none No 1.8.0 mclass_policy Media class usage policy (see discussion for values) capacity_only No 1.8.0 Below are additional details on setting several of these KVS parameters. Key Prefix Length (pfx_len) The HSE data model requires that a KVS store either multi-segment keys, with the KVS pfx_len parameter set to the key prefix length, or unsegmented keys, with the KVS pfx_len parameter set to zero (0). Applications must adhere to this data model, and its associated best practices , to achieve maximum performance with HSE. Media Class Usage (mclass_policy) The mpool storage for a KVDB can be configured with both the required capacity media class and an optional staging media class. For example, an mpool might be created with a capacity media class on a volume backed by value-oriented SATA QLC SSDs, and a staging media class on a volume backed by high-end NVMe TLC SSDs. See the mpool Wiki for complete information on configuring and managing mpool storage. The media class usage policy for a KVS defines how the KVS is stored when a staging media class is present. The KVS, or more precisely the KV data in that KVS, can be either pinned to a particular media class, or tiered from the staging media class to the capacity media class as it ages, as specified by its mclass_policy parameter. mclass_policy Value Description Staging Storage for Tiering capacity_only KVS pinned to capacity media class staging_only KVS pinned to staging media class staging_min_capacity KVS keys and values tiered Up to 100GB staging_max_capacity KVS keys pinned to staging media class with values tiered Up to 100GB + key data + 20% of value data Of the two tiering options, staging_max_capacity will in general yield the highest throughput, lowest latency, and least write-amplification in the capacity media class. The amount of staging storage consumed when tiering depends on many factors, but the estimates above provide reasonable guidelines for planning. Regardless of the media class usage policies selected for the KVS in a KVDB, always apply the best practices for configuring mpool storage. Also be aware that when the staging media class is present, a KVDB will consume up to approximately 50GB of space in it independent of the media class usage policies selected for its KVS. Note If no staging media class is present, and an mclass_policy value other than capacity_only is specified, a warning is logged and capacity_only is applied. Configuration Files The parameters for a KVDB, and any KVS within that KVDB, can be specified in a configuration file using YAML format. The syntax is mostly easily defined by example. api_version: 1 kvdb: dur_intvl_ms: 200 log_lvl: 6 throttle_init_policy: light kvs: pfx_len: 8 value_compression: lz4 mclass_policy: capacity_only kvs.kvs1: pfx_len: 0 mclass_policy: staging_max_capacity This file specifies a dur_intvl_ms of 200, a log_lvl of 6, and a throttle_init_policy of light for the KVDB. Every KVS created in the KVDB will have a pfx_len of 8 bytes, a value_compression of lz4 , and an mclass_policy of capacity_only . The single exception is the KVS named kvs1 , which will have a pfx_len of 0 bytes, and an mclass_policy of staging_max_capacity . Unspecified parameters get their default values. Values for parameters which are persistent, and which have already been applied, are ignored. The only required key is api_version specifying the schema, which is currently 1. When a configuration file is passed to the hse CLI, any parameters specified directly on the command line will override those in the configuration file.","title":"Create a KVDB"},{"location":"gs/create-kvdb/#create-a-kvdb","text":"After configuring an mpool for HSE storage, you can create (store) a KVDB in the mpool using the hse CLI. Continuing with the prior example , we create a KVDB in the mpool mydb . $ hse kvdb create mydb $ hse kvdb list kvdbs: - name: mydb label: raw The name and label of the KVDB are simply the name and label of the mpool that stores it. In a later example, we will update the label to something more descriptive. At this point, the KVDB is ready to use with one of the applications that has already been integrated with HSE, or with your own application. The remainder of this section defines HSE configuration parameters, and provides further hse CLI usage examples. For applications we have integrated with HSE, detailed configuration information is provided in their individual sections of this documentation.","title":"Create a KVDB"},{"location":"gs/create-kvdb/#configuration-parameters","text":"HSE configuration parameters can be specified in several ways. Directly in hse CLI commands as <param>=<value> pairs From a configuration file passed to the hse CLI with the -c option In HSE API calls using functions for specifying parameters directly or reading them from a configuration file The most common (and preferred) method is for an application to use HSE API calls to read parameter values from a configuration file, and then to augment or override those values as appropriate for that application. HSE parameters apply to either a KVDB, or to a key-value store (KVS) within a KVDB. A KVS in a KVDB is analogous to a table in a relational database in that each KVS stores an independent collection of key-value (KV) pairs. These and other HSE concepts are described in the section on developing HSE applications. When using the hse CLI, only persistent parameters need to be specified. Specifying non-persistent parameters has no effect. The KVDB and KVS parameter tables below specify the HSE version at which a parameter was introduced. If the version column is blank, the parameter has been available since the initial release. Note The configuration parameters listed below are part of the stable HSE API. Configuration files installed for applications that have been integrated with HSE may use experimental parameters.","title":"Configuration Parameters"},{"location":"gs/create-kvdb/#kvdb-parameters","text":"Parameter Description Default Persistent Version dur_intvl_ms Max time data is cached before flushing to media (ms) 500 No log_lvl Syslog severity level (0=emergency; 7=debug) 7 No read_only Read-only mode enabled (0=disabled; 1=enabled) 0 No throttle_init_policy Ingest throttle at startup (light; medium; default) default No 1.8.0 Below are additional details on setting several of these KVDB parameters.","title":"KVDB Parameters"},{"location":"gs/create-kvdb/#durability-interval-dur_intvl_ms","text":"The durability interval for a KVDB is the frequency at which cached updates from committed transactions and stand-alone KV operations are flushed to stable storage. This interval is specified by the dur_intvl_ms parameter in units of milliseconds. The concepts section provides additional discussion on HSE durability controls .","title":"Durability Interval (dur_intvl_ms)"},{"location":"gs/create-kvdb/#initial-throttle-throttle_init_policy","text":"On startup, a KVDB throttles the rate at which it processes puts and deletes of KV pairs, referred to as the ingest rate. KVDB increases the ingest rate until it reaches the maximum sustainable value for the underlying mpool storage. This ramp-up process can take on the order of 50 to 200 seconds . For benchmarks, this initial throttling can greatly distort results. In production environments, this initial throttling may impact the time before a service is fully operational. The throttle_init_policy parameter can be used to achieve the maximum ingest rate in far less time. It specifies a relative initial throttling value of light , medium , or default (maximum) throttling. Setting the throttle_init_policy parameter improperly for the underlying mpool storage can cause the durability interval ( dur_intvl_ms ) to be violated, or internal indexing structures to become unbalanced until KVDB determines the maximum sustainable ingest rate for the mpool. The mpool_profile tool is provided to determine an appropriate throttle_init_policy setting for an mpool as follows $ /opt/hse/bin/mpool_profile -v mydb Use the output of mpool_profile to specify the throttle_init_policy value for a KVDB. If the mpool configuration changes, for example a staging media class is added, rerun mpool_profile as the throttle_init_policy value may change. Tip mpool_profile may run for several minutes before producing output.","title":"Initial Throttle (throttle_init_policy)"},{"location":"gs/create-kvdb/#kvs-parameters","text":"Parameter Description Default Persistent Version pfx_len Key prefix length (bytes) 0 Yes value_compression Value compression method (lz4; none) none No 1.8.0 mclass_policy Media class usage policy (see discussion for values) capacity_only No 1.8.0 Below are additional details on setting several of these KVS parameters.","title":"KVS Parameters"},{"location":"gs/create-kvdb/#key-prefix-length-pfx_len","text":"The HSE data model requires that a KVS store either multi-segment keys, with the KVS pfx_len parameter set to the key prefix length, or unsegmented keys, with the KVS pfx_len parameter set to zero (0). Applications must adhere to this data model, and its associated best practices , to achieve maximum performance with HSE.","title":"Key Prefix Length (pfx_len)"},{"location":"gs/create-kvdb/#media-class-usage-mclass_policy","text":"The mpool storage for a KVDB can be configured with both the required capacity media class and an optional staging media class. For example, an mpool might be created with a capacity media class on a volume backed by value-oriented SATA QLC SSDs, and a staging media class on a volume backed by high-end NVMe TLC SSDs. See the mpool Wiki for complete information on configuring and managing mpool storage. The media class usage policy for a KVS defines how the KVS is stored when a staging media class is present. The KVS, or more precisely the KV data in that KVS, can be either pinned to a particular media class, or tiered from the staging media class to the capacity media class as it ages, as specified by its mclass_policy parameter. mclass_policy Value Description Staging Storage for Tiering capacity_only KVS pinned to capacity media class staging_only KVS pinned to staging media class staging_min_capacity KVS keys and values tiered Up to 100GB staging_max_capacity KVS keys pinned to staging media class with values tiered Up to 100GB + key data + 20% of value data Of the two tiering options, staging_max_capacity will in general yield the highest throughput, lowest latency, and least write-amplification in the capacity media class. The amount of staging storage consumed when tiering depends on many factors, but the estimates above provide reasonable guidelines for planning. Regardless of the media class usage policies selected for the KVS in a KVDB, always apply the best practices for configuring mpool storage. Also be aware that when the staging media class is present, a KVDB will consume up to approximately 50GB of space in it independent of the media class usage policies selected for its KVS. Note If no staging media class is present, and an mclass_policy value other than capacity_only is specified, a warning is logged and capacity_only is applied.","title":"Media Class Usage (mclass_policy)"},{"location":"gs/create-kvdb/#configuration-files","text":"The parameters for a KVDB, and any KVS within that KVDB, can be specified in a configuration file using YAML format. The syntax is mostly easily defined by example. api_version: 1 kvdb: dur_intvl_ms: 200 log_lvl: 6 throttle_init_policy: light kvs: pfx_len: 8 value_compression: lz4 mclass_policy: capacity_only kvs.kvs1: pfx_len: 0 mclass_policy: staging_max_capacity This file specifies a dur_intvl_ms of 200, a log_lvl of 6, and a throttle_init_policy of light for the KVDB. Every KVS created in the KVDB will have a pfx_len of 8 bytes, a value_compression of lz4 , and an mclass_policy of capacity_only . The single exception is the KVS named kvs1 , which will have a pfx_len of 0 bytes, and an mclass_policy of staging_max_capacity . Unspecified parameters get their default values. Values for parameters which are persistent, and which have already been applied, are ignored. The only required key is api_version specifying the schema, which is currently 1. When a configuration file is passed to the hse CLI, any parameters specified directly on the command line will override those in the configuration file.","title":"Configuration Files"},{"location":"gs/install-packages/","text":"Install from Packages HSE can be installed from release packages in a few simple steps. Tip Follow the instructions to install all dependencies for your platform, and to build and install the mpool-kmod repo from source, before installing the mpool repo release packages, followed by the hse repo release packages. Download and Install mpool Packages Download the latest mpool and mpool-devel packages for your platform, which contain the user-space API library, CLI, and header files for the mpool kernel module. Package names start with mpool*X.Y.Z and mpool-devel*X.Y.Z , where X.Y.Z is the version. Install the mpool packages for your platform as follows. RHEL 8 $ sudo dnf install ./mpool-X.Y.Z*.rpm $ sudo dnf install ./mpool-devel-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./mpool_X.Y.Z*.deb $ sudo apt-get install ./mpool-devel_X.Y.Z*.deb RHEL 7 $ sudo yum install ./mpool-X.Y.Z*.rpm $ sudo yum install ./mpool-devel-X.Y.Z*.rpm Download and Install hse Packages Download the latest hse and hse-devel packages for your platform, which contain the HSE library, CLI, and header files. Package names start with hse*X.Y.Z and hse-devel*X.Y.Z , where X.Y.Z is the version. Install the hse packages for your platform as follows. RHEL 8 $ sudo dnf install ./hse-X.Y.Z*.rpm $ sudo dnf install ./hse-devel-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./hse_X.Y.Z*.deb $ sudo apt-get install ./hse-devel_X.Y.Z*.deb RHEL 7 $ sudo yum install ./hse-X.Y.Z*.rpm $ sudo yum install ./hse-devel-X.Y.Z*.rpm","title":"Install from Packages"},{"location":"gs/install-packages/#install-from-packages","text":"HSE can be installed from release packages in a few simple steps. Tip Follow the instructions to install all dependencies for your platform, and to build and install the mpool-kmod repo from source, before installing the mpool repo release packages, followed by the hse repo release packages.","title":"Install from Packages"},{"location":"gs/install-packages/#download-and-install-mpool-packages","text":"Download the latest mpool and mpool-devel packages for your platform, which contain the user-space API library, CLI, and header files for the mpool kernel module. Package names start with mpool*X.Y.Z and mpool-devel*X.Y.Z , where X.Y.Z is the version. Install the mpool packages for your platform as follows. RHEL 8 $ sudo dnf install ./mpool-X.Y.Z*.rpm $ sudo dnf install ./mpool-devel-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./mpool_X.Y.Z*.deb $ sudo apt-get install ./mpool-devel_X.Y.Z*.deb RHEL 7 $ sudo yum install ./mpool-X.Y.Z*.rpm $ sudo yum install ./mpool-devel-X.Y.Z*.rpm","title":"Download and Install mpool Packages"},{"location":"gs/install-packages/#download-and-install-hse-packages","text":"Download the latest hse and hse-devel packages for your platform, which contain the HSE library, CLI, and header files. Package names start with hse*X.Y.Z and hse-devel*X.Y.Z , where X.Y.Z is the version. Install the hse packages for your platform as follows. RHEL 8 $ sudo dnf install ./hse-X.Y.Z*.rpm $ sudo dnf install ./hse-devel-X.Y.Z*.rpm Ubuntu 18.04 $ sudo apt-get install ./hse_X.Y.Z*.deb $ sudo apt-get install ./hse-devel_X.Y.Z*.deb RHEL 7 $ sudo yum install ./hse-X.Y.Z*.rpm $ sudo yum install ./hse-devel-X.Y.Z*.rpm","title":"Download and Install hse Packages"},{"location":"gs/install-source/","text":"Install from Source HSE can be built and installed from source in a few simple steps. Tip Build and install from repo mpool-kmod , then mpool , and then hse . Install Dependencies Install dependencies for your platform. RHEL 8 Install the following tools, libraries, and headers. $ sudo dnf install \"@Development Tools\" cmake kernel-headers-$(uname -r) kernel-devel-$(uname -r) elfutils-libelf-devel sos rsync libuuid-devel openssl-devel java-1.8.0-openjdk-headless java-1.8.0-openjdk-devel libmicrohttpd libmicrohttpd-devel userspace-rcu userspace-rcu-devel lz4 lz4-devel Ubuntu 18.04 Download CMake 3.13.2 or later and install on your system. Install the following tools, libraries, and headers. $ sudo apt-get install build-essential libblkid-dev gettext autopoint autoconf bison libtool pkg-config openjdk-8-jdk libmicrohttpd-dev liburcu-dev libyaml-dev liblz4-1 liblz4-dev curl RHEL 7 Download CMake 3.11.4 or later and install on your system. Install the following tools, libraries, and headers. $ sudo yum install \"@Development Tools\" kernel-headers-$(uname -r) kernel-devel-$(uname -r) elfutils-libelf-devel sos rsync libuuid-devel openssl-devel java-1.8.0-openjdk-headless java-1.8.0-openjdk-devel libmicrohttpd libmicrohttpd-devel userspace-rcu userspace-rcu-devel lz4 lz4-devel Build mpool-kmod Repo Clone the latest release tag from the mpool-kmod repo , which contains the mpool kernel module source. Releases are named rX.Y.Z , where X.Y.Z is the version. For example $ git clone https://github.com/hse-project/mpool-kmod.git $ cd mpool-kmod $ git checkout rX.Y.Z Build and install for your platform as follows. RHEL 8 $ make package $ sudo dnf install ./builds/$(uname -n)/$(uname -r)/rpm/release/kmod-mpool-$(uname -r)-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ . Ubuntu 18.04 $ make package $ sudo apt-get install ./builds/$(uname -n)/$(uname -r)/deb/release/kmod-mpool-$(uname -r)_X.Y.Z*.deb Note Ubuntu requires release r1.8.0 or later. RHEL 7 $ make package $ sudo yum install ./builds/$(uname -n)/$(uname -r)/rpm/release/kmod-mpool-$(uname -r)-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ . Validate mpool Loaded Validate that the kernel module loaded correctly by confirming that mpool appears in the output of the lsmod command. If it does not appear, try manually loading the kernel module using sudo modprobe mpool . If you update your kernel, you will need to re-build and install the mpool kernel module. Build mpool Repo Clone the latest release tag from the mpool repo , which contains the mpool user-space API and CLI source. Releases are named rX.Y.Z , where X.Y.Z is the version. For example $ git clone https://github.com/hse-project/mpool.git $ cd mpool $ git checkout rX.Y.Z Build and install for your platform as follows. RHEL 8 $ make package $ sudo dnf install ./builds/$(uname -n)/rpm/release/mpool-X.Y.Z*.rpm $ sudo dnf install ./builds/$(uname -n)/rpm/release/mpool-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release*/ . Ubuntu 18.04 $ make package $ sudo apt-get install ./builds/$(uname -n)/deb/release/mpool_X.Y.Z*.deb $ sudo apt-get install ./builds/$(uname -n)/deb/release/mpool-devel_X.Y.Z*.deb Note Ubuntu requires release r1.8.0 or later. RHEL 7 $ make package $ sudo yum install ./builds/$(uname -n)/rpm/release/mpool-X.Y.Z*.rpm $ sudo yum install ./builds/$(uname -n)/rpm/release/mpool-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release*/ . Build hse Repo Clone the latest release tag from the hse repo , which contains the HSE source. Releases are named rX.Y.Z , where X.Y.Z is the version. For example $ git clone https://github.com/hse-project/hse.git $ cd hse $ git checkout rX.Y.Z Build and install for your platform as follows. RHEL 8 $ make package $ sudo dnf install ./builds/$(uname -n)/rpm/release/hse-X.Y.Z*.rpm $ sudo dnf install ./builds/$(uname -n)/rpm/release/hse-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ . Ubuntu 18.04 $ make package $ sudo apt-get install ./builds/$(uname -n)/deb/release/hse_X.Y.Z*.deb $ sudo apt-get install ./builds/$(uname -n)/deb/release/hse-devel_X.Y.Z*.deb Note Ubuntu requires release r1.8.0 or later. RHEL 7 $ make package $ sudo yum install ./builds/$(uname -n)/rpm/release/hse-X.Y.Z*.rpm $ sudo yum install ./builds/$(uname -n)/rpm/release/hse-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ .","title":"Install from Source"},{"location":"gs/install-source/#install-from-source","text":"HSE can be built and installed from source in a few simple steps. Tip Build and install from repo mpool-kmod , then mpool , and then hse .","title":"Install from Source"},{"location":"gs/install-source/#install-dependencies","text":"Install dependencies for your platform. RHEL 8 Install the following tools, libraries, and headers. $ sudo dnf install \"@Development Tools\" cmake kernel-headers-$(uname -r) kernel-devel-$(uname -r) elfutils-libelf-devel sos rsync libuuid-devel openssl-devel java-1.8.0-openjdk-headless java-1.8.0-openjdk-devel libmicrohttpd libmicrohttpd-devel userspace-rcu userspace-rcu-devel lz4 lz4-devel Ubuntu 18.04 Download CMake 3.13.2 or later and install on your system. Install the following tools, libraries, and headers. $ sudo apt-get install build-essential libblkid-dev gettext autopoint autoconf bison libtool pkg-config openjdk-8-jdk libmicrohttpd-dev liburcu-dev libyaml-dev liblz4-1 liblz4-dev curl RHEL 7 Download CMake 3.11.4 or later and install on your system. Install the following tools, libraries, and headers. $ sudo yum install \"@Development Tools\" kernel-headers-$(uname -r) kernel-devel-$(uname -r) elfutils-libelf-devel sos rsync libuuid-devel openssl-devel java-1.8.0-openjdk-headless java-1.8.0-openjdk-devel libmicrohttpd libmicrohttpd-devel userspace-rcu userspace-rcu-devel lz4 lz4-devel","title":"Install Dependencies"},{"location":"gs/install-source/#build-mpool-kmod-repo","text":"Clone the latest release tag from the mpool-kmod repo , which contains the mpool kernel module source. Releases are named rX.Y.Z , where X.Y.Z is the version. For example $ git clone https://github.com/hse-project/mpool-kmod.git $ cd mpool-kmod $ git checkout rX.Y.Z Build and install for your platform as follows. RHEL 8 $ make package $ sudo dnf install ./builds/$(uname -n)/$(uname -r)/rpm/release/kmod-mpool-$(uname -r)-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ . Ubuntu 18.04 $ make package $ sudo apt-get install ./builds/$(uname -n)/$(uname -r)/deb/release/kmod-mpool-$(uname -r)_X.Y.Z*.deb Note Ubuntu requires release r1.8.0 or later. RHEL 7 $ make package $ sudo yum install ./builds/$(uname -n)/$(uname -r)/rpm/release/kmod-mpool-$(uname -r)-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ .","title":"Build mpool-kmod Repo"},{"location":"gs/install-source/#validate-mpool-loaded","text":"Validate that the kernel module loaded correctly by confirming that mpool appears in the output of the lsmod command. If it does not appear, try manually loading the kernel module using sudo modprobe mpool . If you update your kernel, you will need to re-build and install the mpool kernel module.","title":"Validate mpool Loaded"},{"location":"gs/install-source/#build-mpool-repo","text":"Clone the latest release tag from the mpool repo , which contains the mpool user-space API and CLI source. Releases are named rX.Y.Z , where X.Y.Z is the version. For example $ git clone https://github.com/hse-project/mpool.git $ cd mpool $ git checkout rX.Y.Z Build and install for your platform as follows. RHEL 8 $ make package $ sudo dnf install ./builds/$(uname -n)/rpm/release/mpool-X.Y.Z*.rpm $ sudo dnf install ./builds/$(uname -n)/rpm/release/mpool-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release*/ . Ubuntu 18.04 $ make package $ sudo apt-get install ./builds/$(uname -n)/deb/release/mpool_X.Y.Z*.deb $ sudo apt-get install ./builds/$(uname -n)/deb/release/mpool-devel_X.Y.Z*.deb Note Ubuntu requires release r1.8.0 or later. RHEL 7 $ make package $ sudo yum install ./builds/$(uname -n)/rpm/release/mpool-X.Y.Z*.rpm $ sudo yum install ./builds/$(uname -n)/rpm/release/mpool-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release*/ .","title":"Build mpool Repo"},{"location":"gs/install-source/#build-hse-repo","text":"Clone the latest release tag from the hse repo , which contains the HSE source. Releases are named rX.Y.Z , where X.Y.Z is the version. For example $ git clone https://github.com/hse-project/hse.git $ cd hse $ git checkout rX.Y.Z Build and install for your platform as follows. RHEL 8 $ make package $ sudo dnf install ./builds/$(uname -n)/rpm/release/hse-X.Y.Z*.rpm $ sudo dnf install ./builds/$(uname -n)/rpm/release/hse-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ . Ubuntu 18.04 $ make package $ sudo apt-get install ./builds/$(uname -n)/deb/release/hse_X.Y.Z*.deb $ sudo apt-get install ./builds/$(uname -n)/deb/release/hse-devel_X.Y.Z*.deb Note Ubuntu requires release r1.8.0 or later. RHEL 7 $ make package $ sudo yum install ./builds/$(uname -n)/rpm/release/hse-X.Y.Z*.rpm $ sudo yum install ./builds/$(uname -n)/rpm/release/hse-devel-X.Y.Z*.rpm Note For release r1.7.1 use package path ./builds/release/ .","title":"Build hse Repo"},{"location":"gs/manage-kvdb/","text":"Manage a KVDB HSE requires minimal management beyond configuring mpool storage, creating a KVDB, and optionally defining an associated configuration file as previously described. The following examples illustrate the few other aspects of KVS and KVDB management. Managing KVS Most often a client application will use the HSE API to create and manage the one or more KVS it needs for storing collections of KV pairs in accordance with the HSE data model . However, KVS can also be managed from the hse CLI. Create a KVS Continuing with our example KVDB, we create three KVS. $ hse kvs create mydb/docData docData.pfx_len=0 $ hse kvs create mydb/docIdx docIdx.pfx_len=16 $ hse kvs create mydb/docIdxAlt docIdxAlt.pfx_len=12 $ hse kvdb list -v kvdbs: - name: mydb label: raw kvslist: - mydb/docData - mydb/docIdx - mydb/docIdxAlt Destroy a KVS Similarly, we can remove a KVS from the KVDB, along with any KV data it stores. $ hse kvs destroy mydb/docIdxAlt $ hse kvdb list -v kvdbs: - name: mydb label: raw kvslist: - mydb/docData - mydb/docIdx Managing KVDB Most KVDB management is related to its lifecycle or storage. View Parameters for a KVDB The parameters for a KVDB can be specified via multiple sources, as previously discussed. You can view the parameters in effect for an active KVDB, including all of its KVS, as follows. $ hse kvdb params mydb Tip For this command to work, the KVDB must be open by a client application. Rename or Label a KVDB As previously noted, the name and label of a KVDB are simply the name and label of the mpool that stores it. Hence, the mpool CLI is used to change the name or label of a KVDB. Note that an mpool (and hence KVDB) must be deactivated to rename it. $ sudo mpool deactivate mydb $ sudo mpool rename mydb docDB $ sudo mpool activate docDB $ sudo mpool set docDB label=Weather_Docs $ hse kvdb list -v kvdbs: - name: docDB label: Weather_Docs kvslist: - docDB/docData - docDB/docIdx Compact a KVDB HSE implements the physical removal of logically deleted data as a background operation. Physical removal can also be forced from the hse CLI. This operation can take several minutes depending on the amount of data stored in the KVDB, among other factors, and so a timeout can be specified. $ hse kvdb compact --timeout 120 docDB If a KVDB is in use by an application, the compaction operation may continue past the timeout value (specified or default). In this case, the status of the compaction can be queried. $ hse kvdb compact --status docDB The compaction can also be canceled. $ hse kvdb compact --cancel docDB Destroy a KVDB Finally, a KVDB is deleted by destroying the mpool that stores it. $ sudo mpool destroy docDB $ hse kvdb list No KVDBs found","title":"Manage a KVDB"},{"location":"gs/manage-kvdb/#manage-a-kvdb","text":"HSE requires minimal management beyond configuring mpool storage, creating a KVDB, and optionally defining an associated configuration file as previously described. The following examples illustrate the few other aspects of KVS and KVDB management.","title":"Manage a KVDB"},{"location":"gs/manage-kvdb/#managing-kvs","text":"Most often a client application will use the HSE API to create and manage the one or more KVS it needs for storing collections of KV pairs in accordance with the HSE data model . However, KVS can also be managed from the hse CLI.","title":"Managing KVS"},{"location":"gs/manage-kvdb/#create-a-kvs","text":"Continuing with our example KVDB, we create three KVS. $ hse kvs create mydb/docData docData.pfx_len=0 $ hse kvs create mydb/docIdx docIdx.pfx_len=16 $ hse kvs create mydb/docIdxAlt docIdxAlt.pfx_len=12 $ hse kvdb list -v kvdbs: - name: mydb label: raw kvslist: - mydb/docData - mydb/docIdx - mydb/docIdxAlt","title":"Create a KVS"},{"location":"gs/manage-kvdb/#destroy-a-kvs","text":"Similarly, we can remove a KVS from the KVDB, along with any KV data it stores. $ hse kvs destroy mydb/docIdxAlt $ hse kvdb list -v kvdbs: - name: mydb label: raw kvslist: - mydb/docData - mydb/docIdx","title":"Destroy a KVS"},{"location":"gs/manage-kvdb/#managing-kvdb","text":"Most KVDB management is related to its lifecycle or storage.","title":"Managing KVDB"},{"location":"gs/manage-kvdb/#view-parameters-for-a-kvdb","text":"The parameters for a KVDB can be specified via multiple sources, as previously discussed. You can view the parameters in effect for an active KVDB, including all of its KVS, as follows. $ hse kvdb params mydb Tip For this command to work, the KVDB must be open by a client application.","title":"View Parameters for a KVDB"},{"location":"gs/manage-kvdb/#rename-or-label-a-kvdb","text":"As previously noted, the name and label of a KVDB are simply the name and label of the mpool that stores it. Hence, the mpool CLI is used to change the name or label of a KVDB. Note that an mpool (and hence KVDB) must be deactivated to rename it. $ sudo mpool deactivate mydb $ sudo mpool rename mydb docDB $ sudo mpool activate docDB $ sudo mpool set docDB label=Weather_Docs $ hse kvdb list -v kvdbs: - name: docDB label: Weather_Docs kvslist: - docDB/docData - docDB/docIdx","title":"Rename or Label a KVDB"},{"location":"gs/manage-kvdb/#compact-a-kvdb","text":"HSE implements the physical removal of logically deleted data as a background operation. Physical removal can also be forced from the hse CLI. This operation can take several minutes depending on the amount of data stored in the KVDB, among other factors, and so a timeout can be specified. $ hse kvdb compact --timeout 120 docDB If a KVDB is in use by an application, the compaction operation may continue past the timeout value (specified or default). In this case, the status of the compaction can be queried. $ hse kvdb compact --status docDB The compaction can also be canceled. $ hse kvdb compact --cancel docDB","title":"Compact a KVDB"},{"location":"gs/manage-kvdb/#destroy-a-kvdb","text":"Finally, a KVDB is deleted by destroying the mpool that stores it. $ sudo mpool destroy docDB $ hse kvdb list No KVDBs found","title":"Destroy a KVDB"},{"location":"gs/sys-requirements/","text":"System Requirements The following are system requirements for successfully deploying HSE in your environment. Hardware Requirements Hardware requirements are largely dictated by the application embedding HSE, and the amount of data stored. The following guidelines are based on experience with previous application integrations. CPU : x86_64; 3.0 GHz or higher; 16 threads or more Memory : 32 GB or more Storage : SSD volumes only ; use NVMe for best performance For systems with multiple direct-attached SSDs, performance can be significantly improved by configuring the SSDs to be balanced across NUMA nodes. Tools like lstopo can be helpful in creating and verifying a balanced configuration. Operating System Requirements Linux\u00ae 64-bit operating system distribution Red Hat\u00ae Enterprise Linux\u00ae 8.2 (RHEL 8.2) Ubuntu\u00ae 18.04.4 LTS Red Hat\u00ae Enterprise Linux\u00ae 7.9 (RHEL 7.9), which received only limited release testing HSE, and the mpool component it depends on, may build and run on other Linux 64-bit distributions, but only those listed above have been tested with the latest release. Mpool Requirements Mpool (object storage media pool) is a loadable kernel module that implements an object storage device interface on SSD volumes. HSE stores its data in an mpool, versus a file system or raw volume. Hence, mpool must be installed and configured on the system as described in this documentation. Version Requirements You must install a supported combination of HSE and mpool releases from the hse , mpool , and mpool-kmod repos, along with any prerequisites. Please review the release notes for details.","title":"System Requirements"},{"location":"gs/sys-requirements/#system-requirements","text":"The following are system requirements for successfully deploying HSE in your environment.","title":"System Requirements"},{"location":"gs/sys-requirements/#hardware-requirements","text":"Hardware requirements are largely dictated by the application embedding HSE, and the amount of data stored. The following guidelines are based on experience with previous application integrations. CPU : x86_64; 3.0 GHz or higher; 16 threads or more Memory : 32 GB or more Storage : SSD volumes only ; use NVMe for best performance For systems with multiple direct-attached SSDs, performance can be significantly improved by configuring the SSDs to be balanced across NUMA nodes. Tools like lstopo can be helpful in creating and verifying a balanced configuration.","title":"Hardware Requirements"},{"location":"gs/sys-requirements/#operating-system-requirements","text":"Linux\u00ae 64-bit operating system distribution Red Hat\u00ae Enterprise Linux\u00ae 8.2 (RHEL 8.2) Ubuntu\u00ae 18.04.4 LTS Red Hat\u00ae Enterprise Linux\u00ae 7.9 (RHEL 7.9), which received only limited release testing HSE, and the mpool component it depends on, may build and run on other Linux 64-bit distributions, but only those listed above have been tested with the latest release.","title":"Operating System Requirements"},{"location":"gs/sys-requirements/#mpool-requirements","text":"Mpool (object storage media pool) is a loadable kernel module that implements an object storage device interface on SSD volumes. HSE stores its data in an mpool, versus a file system or raw volume. Hence, mpool must be installed and configured on the system as described in this documentation.","title":"Mpool Requirements"},{"location":"gs/sys-requirements/#version-requirements","text":"You must install a supported combination of HSE and mpool releases from the hse , mpool , and mpool-kmod repos, along with any prerequisites. Please review the release notes for details.","title":"Version Requirements"},{"location":"help/benchmarking/","text":"Benchmarking The following are tips for benchmarking an HSE client application, whether one of your own, or one we have already integrated such as YCSB or MongoDB . If developing your own application, review the section on HSE concepts and follow HSE client best practices . Mpool Storage Configure mpool storage for the client application on a volume with performance characteristics similar to what will be observed in production. Follow mpool configuration best practices for HSE storage. Use the mpool_profile tool to determine the appropriate KVDB throttle_init_policy value to use with the mpool. Configuration Parameters Create an HSE configuration file for the client application with at least the following parameter settings: kvdb.throttle_init_policy with a value determined by mpool_profile kvdb.dur_intvl_ms with a value appropriate for the application kvs.value_compression with a value of lz4 , unless the application performs its own value compression","title":"Benchmarking Tips"},{"location":"help/benchmarking/#benchmarking","text":"The following are tips for benchmarking an HSE client application, whether one of your own, or one we have already integrated such as YCSB or MongoDB . If developing your own application, review the section on HSE concepts and follow HSE client best practices .","title":"Benchmarking"},{"location":"help/benchmarking/#mpool-storage","text":"Configure mpool storage for the client application on a volume with performance characteristics similar to what will be observed in production. Follow mpool configuration best practices for HSE storage. Use the mpool_profile tool to determine the appropriate KVDB throttle_init_policy value to use with the mpool.","title":"Mpool Storage"},{"location":"help/benchmarking/#configuration-parameters","text":"Create an HSE configuration file for the client application with at least the following parameter settings: kvdb.throttle_init_policy with a value determined by mpool_profile kvdb.dur_intvl_ms with a value appropriate for the application kvs.value_compression with a value of lz4 , unless the application performs its own value compression","title":"Configuration Parameters"},{"location":"help/relnotes/","text":"Release Notes These release notes are divided into the following sections: HSE Stack Release Notes YCSB Release Notes MongoDB Release Notes HSE Stack Release Notes You must install a supported combination of HSE and mpool releases from the hse , mpool , and mpool-kmod repos. The table below specifies the supported combinations, referred to as HSE stacks. HSE stack numbering follows the scheme HSE-S.X.Y.Z.M , where X.Y.Z is the hse release number M is the combination of mpool and mpool-kmod releases to use with X.Y.Z HSE Stack hse version mpool version mpool-kmod version HSE-S.1.7.1.1 r1.7.1 r1.7.1 r1.7.1 HSE-S.1.8.0.1 r1.8.0 r1.8.0 r1.8.0 HSE-S.1.9.0.1 r1.9.0 r1.9.0 r1.9.0 HSE-S.1.9.0.1 Includes the following features and enhancements. QoS enhancements resulting in more consistent performance in the presence of quickly changing workloads, client thread counts, and KV pair sizes Increased value compression performance Experimental KVS parameter kvs.vcompmin specifying the minimum length value to compress, which defaults to eight (8) bytes Prior to installing HSE-S.1.9.0.1, ensure that all KVDBs on the system have shutdown cleanly. If you are not certain that a KVDB was shutdown cleanly, you can run the following command to get the KVDB into a clean state. $ hse kvdb compact -s <kvdb> If HSE is upgraded to 1.9.0 with a KVDB in a dirty state then the KVDB may fail to open. In this case, you will need to downgrade HSE to the prior version and run the command above. Do not skip major releases when upgrading HSE. For example, you should upgrade to HSE-S.1.9.0.1 from HSE-S.1.8.0.1. Known Issues Prefix deletes do not detect conflicts with other mutations sharing the target prefix. As a result, snapshot isolation may be violated. HSE API function hse_kvdb_compact_status() is renamed to hse_kvdb_compact_status_get() to disambiguate it from struct hse_kvdb_compact_status . There is no change in semantics. While we strive to keep the HSE API stable and backward compatible across releases, this minor change is needed to accommodate future language bindings. Ubuntu 18.04 may exhibit performance that is 5-30% below that of RHEL 8, depending on workload. Issues Fixed All issues identified in the release notes for HSE-S.1.8.0.1 related to recovery, spurious ENOSPC errors, and the KVDB becoming full. HSE API function hse_kvs_cursor_update() ignored the opspec flag HSE_KVDB_KOP_FLAG_REVERSE , which must match that specified in the corresponding hse_kvs_cursor_create() . The caller will now correctly receive an EINVAL error if these flag values do not match (e.g., if the caller attempts to change the cursor direction). Various issues found in internal testing. HSE-S.1.8.0.1 Includes the following features and enhancements. Media class tiering policies at the KVS level Value compression settings at the KVS level Configurable ingest throttling at startup based on mpool profiling Documented best practices for benchmarking Support for Ubuntu 18.04.4 CPack-based package names Known Issues Ubuntu 18.04 may exhibit performance that is 5-30% below that of RHEL 8, depending on workload. Recovery from an unclean shutdown (e.g., a crash) may leak mpool capacity. Client applications may experience ENOSPC errors when a large number of threads (hundreds) are mutating large values (hundreds of KB). Shutdown the app and restart to clear the error. If the mpool storing a KVDB becomes full, cached updates may be lost. Should this occur, halt the client application, increase the mpool capacity, and then restart the client application. Issues Fixed Snapshot isolation semantics may be violated under certain conditions. Various issues found in internal testing. HSE-S.1.7.1.1 Reissue of initial release to revise package and kernel module naming. Known Issues None Issues Fixed Build of tarball automatically generated by GitHub Failure to load mpool kernel module on SELinux YCSB Release Notes Releases of YCSB with HSE use the numbering scheme rA.B.C.D.E-hse-X.Y.Z , where A.B.C is the YCSB version (e.g., 0.17.0 ) D.E is our YCSB integration version X.Y.Z is the minimum HSE release version required YCSB r0.17.0.2.1-hse-1.8.0 Adds support for Ubuntu 18.04.4. Known Issues None Issues Fixed Various issues found in internal testing. YCSB r0.17.0.2.0-hse-1.7.1 Reissue of initial release of YCSB with HSE to revise package naming. Known Issues None Issues Fixed Build of tarball automatically generated by GitHub MongoDB Release Notes Releases of MongoDB with HSE use the numbering scheme rA.B.C.D.E-hse-X.Y.Z , where A.B.C is the MongoDB version (e.g., 3.4.17 ) D.E is our MongoDB integration version X.Y.Z is the minimum HSE release version required MongoDB r3.4.17.3.0-hse-1.9.0 Includes the following features and enhancements. Bug fixes only This version of MongoDB with HSE does not support the following: compact administration command fsync administration command with the lock option, or the corresponding fsyncUnlock command Read concern \"majority\" SSL on RHEL 8 or Ubuntu 18.04 Known Issues None Issues Fixed Various issues found in internal testing. MongoDB r3.4.17.2.1-hse-1.8.0 Includes the following features and enhancements. Support for Ubuntu 18.04.4 Building from source creates packages This version of MongoDB with HSE does not support the following: compact administration command fsync administration command with the lock option, or the corresponding fsyncUnlock command Read concern \"majority\" SSL on RHEL 8 or Ubuntu 18.04 Known Issues Cannot be used with HSE 1.9.0 or later due to improper use of an HSE API argument which is detected starting with HSE 1.9.0. Issues Fixed Under certain conditions an object may fail to be replicated, or the incorrect object version may be replicated. Various issues found in internal testing. MongoDB r3.4.17.2.0-hse-1.7.1 Reissue of initial release of MongoDB with HSE to revise package naming. This version of MongoDB with HSE does not support the following: compact administration command fsync administration command with the lock option, or the corresponding fsyncUnlock command Read concern \"majority\" SSL on RHEL 8 Known Issues None Issues Fixed Build of tarball automatically generated by GitHub The following build commands are referenced from the MongoDB section of this documentation. Please refer to those instructions. Running these commands results in the same set of binaries in the build directory as are normally generated when compiling the unmodified source code base. No packages are generated. RHEL 8 $ scons CC=/opt/rh/gcc-toolset-9/root/bin/gcc CXX=/opt/rh/gcc-toolset-9/root/bin/g++ OBJCOPY=/opt/rh/gcc-toolset-9/root/bin/objcopy -j$(nproc) --disable-warnings-as-errors --dbg=off --opt=on all MONGO_VERSION=3.4.17 Tip Do not specify --ssl when building for RHEL 8. RHEL 7 $ scl enable rh-mongodb32 'scons CC=/opt/rh/devtoolset-9/root/bin/gcc CXX=/opt/rh/devtoolset-9/root/bin/g++ OBJCOPY=/opt/rh/devtoolset-9/root/bin/objcopy -j$(nproc) --ssl --disable-warnings-as-errors --dbg=off --opt=on all MONGO_VERSION=3.4.17'","title":"Release Notes"},{"location":"help/relnotes/#release-notes","text":"These release notes are divided into the following sections: HSE Stack Release Notes YCSB Release Notes MongoDB Release Notes","title":"Release Notes"},{"location":"help/relnotes/#hse-stack-release-notes","text":"You must install a supported combination of HSE and mpool releases from the hse , mpool , and mpool-kmod repos. The table below specifies the supported combinations, referred to as HSE stacks. HSE stack numbering follows the scheme HSE-S.X.Y.Z.M , where X.Y.Z is the hse release number M is the combination of mpool and mpool-kmod releases to use with X.Y.Z HSE Stack hse version mpool version mpool-kmod version HSE-S.1.7.1.1 r1.7.1 r1.7.1 r1.7.1 HSE-S.1.8.0.1 r1.8.0 r1.8.0 r1.8.0 HSE-S.1.9.0.1 r1.9.0 r1.9.0 r1.9.0","title":"HSE Stack Release Notes"},{"location":"help/relnotes/#hse-s1901","text":"Includes the following features and enhancements. QoS enhancements resulting in more consistent performance in the presence of quickly changing workloads, client thread counts, and KV pair sizes Increased value compression performance Experimental KVS parameter kvs.vcompmin specifying the minimum length value to compress, which defaults to eight (8) bytes Prior to installing HSE-S.1.9.0.1, ensure that all KVDBs on the system have shutdown cleanly. If you are not certain that a KVDB was shutdown cleanly, you can run the following command to get the KVDB into a clean state. $ hse kvdb compact -s <kvdb> If HSE is upgraded to 1.9.0 with a KVDB in a dirty state then the KVDB may fail to open. In this case, you will need to downgrade HSE to the prior version and run the command above. Do not skip major releases when upgrading HSE. For example, you should upgrade to HSE-S.1.9.0.1 from HSE-S.1.8.0.1. Known Issues Prefix deletes do not detect conflicts with other mutations sharing the target prefix. As a result, snapshot isolation may be violated. HSE API function hse_kvdb_compact_status() is renamed to hse_kvdb_compact_status_get() to disambiguate it from struct hse_kvdb_compact_status . There is no change in semantics. While we strive to keep the HSE API stable and backward compatible across releases, this minor change is needed to accommodate future language bindings. Ubuntu 18.04 may exhibit performance that is 5-30% below that of RHEL 8, depending on workload. Issues Fixed All issues identified in the release notes for HSE-S.1.8.0.1 related to recovery, spurious ENOSPC errors, and the KVDB becoming full. HSE API function hse_kvs_cursor_update() ignored the opspec flag HSE_KVDB_KOP_FLAG_REVERSE , which must match that specified in the corresponding hse_kvs_cursor_create() . The caller will now correctly receive an EINVAL error if these flag values do not match (e.g., if the caller attempts to change the cursor direction). Various issues found in internal testing.","title":"HSE-S.1.9.0.1"},{"location":"help/relnotes/#hse-s1801","text":"Includes the following features and enhancements. Media class tiering policies at the KVS level Value compression settings at the KVS level Configurable ingest throttling at startup based on mpool profiling Documented best practices for benchmarking Support for Ubuntu 18.04.4 CPack-based package names Known Issues Ubuntu 18.04 may exhibit performance that is 5-30% below that of RHEL 8, depending on workload. Recovery from an unclean shutdown (e.g., a crash) may leak mpool capacity. Client applications may experience ENOSPC errors when a large number of threads (hundreds) are mutating large values (hundreds of KB). Shutdown the app and restart to clear the error. If the mpool storing a KVDB becomes full, cached updates may be lost. Should this occur, halt the client application, increase the mpool capacity, and then restart the client application. Issues Fixed Snapshot isolation semantics may be violated under certain conditions. Various issues found in internal testing.","title":"HSE-S.1.8.0.1"},{"location":"help/relnotes/#hse-s1711","text":"Reissue of initial release to revise package and kernel module naming. Known Issues None Issues Fixed Build of tarball automatically generated by GitHub Failure to load mpool kernel module on SELinux","title":"HSE-S.1.7.1.1"},{"location":"help/relnotes/#ycsb-release-notes","text":"Releases of YCSB with HSE use the numbering scheme rA.B.C.D.E-hse-X.Y.Z , where A.B.C is the YCSB version (e.g., 0.17.0 ) D.E is our YCSB integration version X.Y.Z is the minimum HSE release version required","title":"YCSB Release Notes"},{"location":"help/relnotes/#ycsb-r017021-hse-180","text":"Adds support for Ubuntu 18.04.4. Known Issues None Issues Fixed Various issues found in internal testing.","title":"YCSB r0.17.0.2.1-hse-1.8.0"},{"location":"help/relnotes/#ycsb-r017020-hse-171","text":"Reissue of initial release of YCSB with HSE to revise package naming. Known Issues None Issues Fixed Build of tarball automatically generated by GitHub","title":"YCSB r0.17.0.2.0-hse-1.7.1"},{"location":"help/relnotes/#mongodb-release-notes","text":"Releases of MongoDB with HSE use the numbering scheme rA.B.C.D.E-hse-X.Y.Z , where A.B.C is the MongoDB version (e.g., 3.4.17 ) D.E is our MongoDB integration version X.Y.Z is the minimum HSE release version required","title":"MongoDB Release Notes"},{"location":"help/relnotes/#mongodb-r341730-hse-190","text":"Includes the following features and enhancements. Bug fixes only This version of MongoDB with HSE does not support the following: compact administration command fsync administration command with the lock option, or the corresponding fsyncUnlock command Read concern \"majority\" SSL on RHEL 8 or Ubuntu 18.04 Known Issues None Issues Fixed Various issues found in internal testing.","title":"MongoDB r3.4.17.3.0-hse-1.9.0"},{"location":"help/relnotes/#mongodb-r341721-hse-180","text":"Includes the following features and enhancements. Support for Ubuntu 18.04.4 Building from source creates packages This version of MongoDB with HSE does not support the following: compact administration command fsync administration command with the lock option, or the corresponding fsyncUnlock command Read concern \"majority\" SSL on RHEL 8 or Ubuntu 18.04 Known Issues Cannot be used with HSE 1.9.0 or later due to improper use of an HSE API argument which is detected starting with HSE 1.9.0. Issues Fixed Under certain conditions an object may fail to be replicated, or the incorrect object version may be replicated. Various issues found in internal testing.","title":"MongoDB r3.4.17.2.1-hse-1.8.0"},{"location":"help/relnotes/#mongodb-r341720-hse-171","text":"Reissue of initial release of MongoDB with HSE to revise package naming. This version of MongoDB with HSE does not support the following: compact administration command fsync administration command with the lock option, or the corresponding fsyncUnlock command Read concern \"majority\" SSL on RHEL 8 Known Issues None Issues Fixed Build of tarball automatically generated by GitHub The following build commands are referenced from the MongoDB section of this documentation. Please refer to those instructions. Running these commands results in the same set of binaries in the build directory as are normally generated when compiling the unmodified source code base. No packages are generated. RHEL 8 $ scons CC=/opt/rh/gcc-toolset-9/root/bin/gcc CXX=/opt/rh/gcc-toolset-9/root/bin/g++ OBJCOPY=/opt/rh/gcc-toolset-9/root/bin/objcopy -j$(nproc) --disable-warnings-as-errors --dbg=off --opt=on all MONGO_VERSION=3.4.17 Tip Do not specify --ssl when building for RHEL 8. RHEL 7 $ scl enable rh-mongodb32 'scons CC=/opt/rh/devtoolset-9/root/bin/gcc CXX=/opt/rh/devtoolset-9/root/bin/g++ OBJCOPY=/opt/rh/devtoolset-9/root/bin/objcopy -j$(nproc) --ssl --disable-warnings-as-errors --dbg=off --opt=on all MONGO_VERSION=3.4.17'","title":"MongoDB r3.4.17.2.0-hse-1.7.1"},{"location":"help/resources/","text":"Resources Resources are available to report a bug, ask a question, or provide feedback on how we can improve HSE or this documentation. Report a Bug You can file an issue to report an HSE bug. When reporting a bug, please attach the output of the following command. $ sudo sosreport --batch If you encounter issues generating a complete report, please try the following. $ sudo sosreport -o hse Ask a Question You can ask a question in the HSE discussions forum using the Q&A category. Participate in Discussions You can discuss all aspects of HSE in the discussions forum using the following categories. Q&A to ask a question Ideas to recommend a feature or enhancement Show and tell to talk about your experiences with HSE General to discuss all other aspects of the project","title":"Resources"},{"location":"help/resources/#resources","text":"Resources are available to report a bug, ask a question, or provide feedback on how we can improve HSE or this documentation.","title":"Resources"},{"location":"help/resources/#report-a-bug","text":"You can file an issue to report an HSE bug. When reporting a bug, please attach the output of the following command. $ sudo sosreport --batch If you encounter issues generating a complete report, please try the following. $ sudo sosreport -o hse","title":"Report a Bug"},{"location":"help/resources/#ask-a-question","text":"You can ask a question in the HSE discussions forum using the Q&A category.","title":"Ask a Question"},{"location":"help/resources/#participate-in-discussions","text":"You can discuss all aspects of HSE in the discussions forum using the following categories. Q&A to ask a question Ideas to recommend a feature or enhancement Show and tell to talk about your experiences with HSE General to discuss all other aspects of the project","title":"Participate in Discussions"}]}