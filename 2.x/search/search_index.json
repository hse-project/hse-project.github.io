{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Heterogeneous-Memory Storage Engine HSE is a fast embeddable key-value store designed for SSDs and persistent memory. HSE optimizes performance and endurance by orchestrating data placement across DRAM and multiple classes of solid-state storage devices. HSE is ideal for powering Databases, Software-Defined Storage (SDS), High-Performance Computing (HPC), Internet of Things (IoT), and Machine Learning (ML). Key Features Rich set of key-value operations Full transactions with snapshot-isolation spanning multiple independent key-value collections Cursors for iterating over snapshot views Data model for optimizing mixed use-case workloads in a single data store Key and value compression Flexible durability controls Configurable data orchestration schemes Native C library that can be embedded in any application Benefits Scales to terabytes of data and hundreds of billions of keys per store Efficiently handles thousands of concurrent operations Dramatically improves throughput, latency, write-amplification, and read-amplification versus common alternatives for many workloads Optionally combines multiple classes of solid-state storage to optimize performance and endurance","title":"Home"},{"location":"#heterogeneous-memory-storage-engine","text":"HSE is a fast embeddable key-value store designed for SSDs and persistent memory. HSE optimizes performance and endurance by orchestrating data placement across DRAM and multiple classes of solid-state storage devices. HSE is ideal for powering Databases, Software-Defined Storage (SDS), High-Performance Computing (HPC), Internet of Things (IoT), and Machine Learning (ML).","title":"Heterogeneous-Memory Storage Engine"},{"location":"#key-features","text":"Rich set of key-value operations Full transactions with snapshot-isolation spanning multiple independent key-value collections Cursors for iterating over snapshot views Data model for optimizing mixed use-case workloads in a single data store Key and value compression Flexible durability controls Configurable data orchestration schemes Native C library that can be embedded in any application","title":"Key Features"},{"location":"#benefits","text":"Scales to terabytes of data and hundreds of billions of keys per store Efficiently handles thousands of concurrent operations Dramatically improves throughput, latency, write-amplification, and read-amplification versus common alternatives for many workloads Optionally combines multiple classes of solid-state storage to optimize performance and endurance","title":"Benefits"},{"location":"dev/apis/","text":"API Reference See the HSE API documentation in the hse.h header file. There are also example applications in the samples directory.","title":"API Reference"},{"location":"dev/apis/#api-reference","text":"See the HSE API documentation in the hse.h header file. There are also example applications in the samples directory.","title":"API Reference"},{"location":"dev/bp/","text":"Best Practices The following are best practices for developing HSE applications. Many of these are covered in greater detail in the discussion of HSE concepts . Keys and KVSs Use a segmented key in the common case where related sets of KV pairs in a KVS are accessed together. Choose a key prefix that groups related KV pairs when keys are sorted lexicographically, and always create the KVS storing these KV pairs with a pfx_len equal to the key prefix length. Choose a key prefix for segmented keys that will take on a modest number of different values over a consecutive sequence of puts. For example, in a sequence of one million put operations, ideally no more than 5% of the keys will have the same key prefix value. Use a different KVS for each collection of KV pairs requiring its own segmented key structure. Create index KVSs to efficiently implement query patterns that cannot be supported with a single segmented key prefix. Use an unsegmented key in the case where there is no relationship between KV pairs in a KVS, and always create the KVS storing these KV pairs with a pfx_len of zero (0). Cursors and Gets Always use get operations when iteration is not required. Gets are significantly faster than cursor seeks. Where iteration is required, use cursors with a KVS storing segmented keys, and with a filter whose length is equal to or greater than the key prefix length for that KVS. Otherwise, cursor performance can be greatly reduced. Use non-transaction cursors for most applications. Transaction cursors exist to support some specialized use cases. Transactions Use transactions when required for application correctness. Otherwise, for best performance open a KVS with transactions disabled (per transactions_enable ) and use non-transaction operations. Application Lifecycle At application startup, call hse_init() to initialize the HSE subsystem within your application. This function should be called only once. From there you can create or open a KVDB and its associated KVSs and perform KV operations. At application shutdown, close all KVSs for a KVDB before closing that KVDB. After all KVDBs are closed, call hse_fini() to completely shutdown the HSE subsystem within your application. Using a signal handler to close resources can also be helpful when trying to handle unexpected application shutdown.","title":"Best Practices"},{"location":"dev/bp/#best-practices","text":"The following are best practices for developing HSE applications. Many of these are covered in greater detail in the discussion of HSE concepts .","title":"Best Practices"},{"location":"dev/bp/#keys-and-kvss","text":"Use a segmented key in the common case where related sets of KV pairs in a KVS are accessed together. Choose a key prefix that groups related KV pairs when keys are sorted lexicographically, and always create the KVS storing these KV pairs with a pfx_len equal to the key prefix length. Choose a key prefix for segmented keys that will take on a modest number of different values over a consecutive sequence of puts. For example, in a sequence of one million put operations, ideally no more than 5% of the keys will have the same key prefix value. Use a different KVS for each collection of KV pairs requiring its own segmented key structure. Create index KVSs to efficiently implement query patterns that cannot be supported with a single segmented key prefix. Use an unsegmented key in the case where there is no relationship between KV pairs in a KVS, and always create the KVS storing these KV pairs with a pfx_len of zero (0).","title":"Keys and KVSs"},{"location":"dev/bp/#cursors-and-gets","text":"Always use get operations when iteration is not required. Gets are significantly faster than cursor seeks. Where iteration is required, use cursors with a KVS storing segmented keys, and with a filter whose length is equal to or greater than the key prefix length for that KVS. Otherwise, cursor performance can be greatly reduced. Use non-transaction cursors for most applications. Transaction cursors exist to support some specialized use cases.","title":"Cursors and Gets"},{"location":"dev/bp/#transactions","text":"Use transactions when required for application correctness. Otherwise, for best performance open a KVS with transactions disabled (per transactions_enable ) and use non-transaction operations.","title":"Transactions"},{"location":"dev/bp/#application-lifecycle","text":"At application startup, call hse_init() to initialize the HSE subsystem within your application. This function should be called only once. From there you can create or open a KVDB and its associated KVSs and perform KV operations. At application shutdown, close all KVSs for a KVDB before closing that KVDB. After all KVDBs are closed, call hse_fini() to completely shutdown the HSE subsystem within your application. Using a signal handler to close resources can also be helpful when trying to handle unexpected application shutdown.","title":"Application Lifecycle"},{"location":"dev/concepts/","text":"Concepts The following describes HSE concepts that are important to understand for developing HSE applications and making effective use of the HSE API. KVDB and KVS HSE provides functions to create and access a key-value database (KVDB). A KVDB comprises one or more named key-value stores (KVS), each of which is an independent collection of key-value (KV) pairs. A KVS is analogous to a table in a relational database. HSE provides the standard KV operators for managing KV pairs stored in a KVS: put, get, and delete. HSE also provides transactions, cursors, prefix deletes, and other advanced features. The HSE data model enables each KVS in a KVDB to be optimized for how the KV pairs it stores will be accessed. Data Model Understanding the HSE data model is fundamental to achieving maximum application performance. Adhering to the best practices of this data model can result in significantly greater performance than might be achieved otherwise. While this data model is simple, it has proven very effective. Key Structure To describe the HSE data model, we define the following terms. key \u2014 a byte string used to uniquely identify a value for storage, retrieval, and deletion in a KVS segmented key \u2014 a key that is logically divided into N segments, N >= 2 , arranged to group related KV pairs when keys are sorted lexicographically unsegmented key \u2014 a key not logically divided into segments For segmented keys, we further define the following. key prefix \u2014 the first K segments, 1 <= K < N , that group related KV pairs when keys are sorted lexicographically key prefix length \u2014 the length of a key prefix in bytes KVS Configuration In the common case where related sets of KV pairs are accessed together, best performance is generally achieved by: defining a segmented key with a key prefix such that KV pairs to be accessed together are grouped (contiguous) when keys are sorted lexicographically, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) equal to the key prefix length of the segmented key. In the case where there is no relationship between KV pairs, best performance is generally achieved by: defining an unsegmented key, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of zero (0). Keep in mind that a KVDB may contain multiple KVSs. So in the case where there are multiple collections of KV pairs, each collection can be stored in a different KVS with a key structure, and corresponding KVS pfx_len parameter, that is appropriate for that collection. This is a powerful capability that enables HSE to optimize storage and access for all KV pairs in a KVDB without compromise. Operation Support HSE provides several advanced operations with features that directly support the HSE data model. These operations are described in detail later, but we briefly discuss how they support the data model here. Cursors are used to iterate over keys in a KVS in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys whose initial bytes match the filter. The primary use for cursors is with a KVS storing segmented keys, where the length of the specified filter is equal to or greater than the key prefix length for that KVS. Used this way, cursors provide efficient iteration over sets of related KV pairs. Prefix deletes are used to atomically remove all KV pairs in a KVS with keys whose initial bytes match a specified filter. The length of the filter must be equal to the pfx_len parameter of the KVS. The primary use for prefix deletes is with a KVS storing segmented keys. This is a powerful capability that enables sets of related KV pairs to be deleted from a KVS in a single operation without cursor iteration. Transactions are used to execute a sequence of KV operations atomically. Transactions support operating on KV pairs in one or more KVSs in a KVDB. This allows storing multiple collections of KV pairs in different KVSs to optimize access, without giving up the ability to operate on any of those KV pairs within the context of a single transaction. Modeling Examples Below we present examples of applying the HSE data model to the real-world problem of storing and analyzing machine-generated data. Specifically, log data captured from datacenter servers. System logs are commonly retrieved from datacenter servers on a periodic basis and stored for both real-time and historical analysis. We examine several ways this data might be modeled, depending on how it will be accessed and managed. Simple Log Storage We start with a simple data model for storing log data in a KVDB. This model uses a single KVS with segmented keys as defined below, and values which are individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 ts Timestamp 16 2 typeID Log type identifier In this and later examples, segment names are for convenience of presentation (they do not exist in the HSE API), and segment lengths are representative. Also, partial or complete keys may be represented as tuples using segment names. For example, (sysId) , (sysID, ts) , or (sysID, ts, typeID) . We define the key prefix to be (sysID) , yielding a key prefix length of 8 bytes. Hence, we would create a KVS for storing these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (sysID) can be used to efficiently iterate over the log records associated with the system sysID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than a (sysID, ts) , and then iterate from there. This data model makes it easy and efficient to search the log records associated with system sysID over an arbitrary time span. However, pruning those log records, for example to retain only those from the past 30 days, requires iterating over all older records and deleting them individually. Next we will look at enhancing this data model to make log record maintenance more efficient. Per-System Epoch-based Log Storage We extend the simple data model from above to include an epoch identifier representing a well-defined time interval. For example, an epoch might be four hours in length, with 6 epochs per day, 42 epochs per week, and so forth. We assume that a log record's timestamp can be mapped to a specific epoch through simple computation. We again use a single KVS but with segmented keys as defined below, and values which are individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 epochID Epoch identifier 16 8 ts Timestamp 24 2 typeID Log type identifier We define the key prefix to be (sysID, epochID) , yielding a key prefix length of 16 bytes. Hence, we would create a KVS for storing these KV pairs with a key prefix length parameter ( pfx_len ) of 16 bytes. With this data model and KVS configuration, a cursor with the filter (sysID, epochID) can be used to efficiently iterate over the log records associated with the system sysID within the epoch epochID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (sysID, epochID, ts) , and then iterate from there within the epoch. With this revised data model, it is still easy and efficient to search the log records associated with system sysID over an arbitrary time span, though it is necessary to configure a cursor to iterate over each epoch of interest (e.g., to cross epoch boundaries). However, we can now prune log records very efficiently by using a single prefix delete to remove all KV pairs with a specified key prefix of (sysID, epochID) . Next we examine a variation on this per-system epoch-based data model. All-Systems Epoch-based Log Storage The previous data model makes it easy and efficient to iterate over the log records associated with a given sysID for a specified epochID . However, to view records from multiple systems for a specified epochID requires a cursor iteration per sysID of interest. We can instead group the log records for all systems within an epoch by using a KVS with segmented keys as defined below, and values which are individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 epochID Epoch identifier 8 8 ts Timestamp 16 8 sysID System identifier 24 2 typeID Log type identifier We define the key prefix to be (epochID) , yielding a key prefix length of 8 bytes. Hence we would create a KVS for storing these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (epochID) can be used to efficiently iterate over the log records for all systems within the epoch epochID in timestamp order. Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (epochID, ts) , and then iterate from there within the epoch. It should be clear that there is some loss of efficiency, versus the prior data model, to obtain all the log records associated with a specified sysID over an arbitrary time span, because records associated with systems other than sysID will be in the cursor's view and must be skipped. However, this is still a reasonably efficient query because it meets the criteria that the length of the cursor filter be equal to or greater than the key prefix length to achieve maximum performance. With this revised data model, pruning log records remains efficient because we can still use a single prefix delete to remove all KV pairs with a specified key prefix of (epochID) . In this case, the records for all systems in the epoch are pruned together. Tip This particular data model provides the opportunity to point out another best practice that can result in significantly greater performance. For a KVS storing segmented keys, it is important that the key prefix specified in put operations takes on a modest number of different values over a consecutive sequence of puts. For this example, that means choosing an epoch that is relatively short versus what one might select with the prior data model. Finally, we will examine an index-based data model for log storage. Index-based Log Storage The prior data models for log storage have the benefit of simplicity in that the KVDB has only a single KVS. However, they may provide less flexibility than required by the application. In this next example, we demonstrate how to use multiple KVSs to effectively index log records. For brevity, we define the key structure for each KVS using the tuple syntax, segment names, and segment lengths adopted in prior examples. KVS Name Key Type Key Key Prefix KVS pfx_len Value logRec segmented (epochID, ts, sysID, typeID) (epochID) 8 Log record content sysIdx segmented (sysID, epochID, ts, typeID) (sysID, epochID) 16 The KVS logRec stores the content of all log records, with each log record uniquely identified by the segmented key (epochID, ts, sysID, typeID) with key prefix (epochID) . Hence we would create KVS logRec with a key prefix length parameter ( pfx_len ) of 8 bytes. The KVS sysIdx is an index for the log records stored in KVS logRec , with a key pefix of (sysID, epochID) . Hence we would create KVS sysIdx with a key prefix length parameter ( pfx_len ) of 16 bytes. Using KVS logRec , a cursor with the filter (epochID) can be used to efficiently iterate over the log records for all systems within the epoch epochID in timestamp order. Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (epochID, ts) , and then iterate from there within the epoch. Using KVS sysIdx , a cursor with the filter (sysID, epochID) can be used to efficiently iterate over the log records associated with the system sysID within the epoch epochID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (sysID, epochID, ts) , and then iterate from there within the epoch. Note that KVS sysIdx does not contain any log record contents, and in fact stores no values at all (i.e., all value lengths are zero). To obtain the contents of a specific log record, there is the added step of a get operation in KVS logRec with key (epochID, ts, sysID, typeID) . The above schema accomplishes the following. Efficient iteration over the log records from all systems in a specified epoch using a cursor with KVS logRec . Efficient iteration over the log records from a specific system in a specified epoch using a cursor with KVS sysIdx . No duplication of log record content. A transaction can be used to atomically put (insert) KV pairs for a given log record in KVS logRec and KVS sysIdx . This guarantees integrity of the sysIdx index. Pruning log records is a bit more complex than the prior data models, bit still relatively straight-forward. To delete all log records for a given epochID , where the epoch is assumed to have passed, you would use a cursor with filter (epochID) to iterate over KVS logRec to build a list of all sysID in the epoch, and then in a single transaction, prefix delete each key prefix (sysID, epochID) from KVS sysIdx , and prefix delete the key prefix (epochID) from KVS logRec . Snapshots HSE uses multiversion concurrency control ( MVCC ) techniques to implement industry-standard snapshot isolation semantics for transactions and cursors. In this model, transactions and cursors operate on KVS snapshots in a KVDB. Conceptually, a KVS snapshot contains KV pairs from all transactions committed, or non-transaction operations completed, at the time the KVS snapshot is taken. A KVS snapshot is ephemeral and ceases to exist when all associated transaction and cursor operations complete. Transactions Transactions are used to execute a sequence of KV operations as a unit of work that is atomic, consistent, isolated, and durable ( ACID ). A transaction may operate on KV pairs in one or more KVSs in a KVDB. When a KVS is opened, the transactions_enable parameter specifies whether or not that KVS supports transactions. This is not a persistent setting in that the KVS may be closed and later reopened in a different mode. The following table specifies the operations that may be performed on a KVS opened with transactions enabled or disabled, where: Read is a query operation, such as get or cursor iteration Update is a mutation operation, such as put, delete, or prefix delete KVS Transactions Enabled KVS Transactions Disabled Transaction Read Transaction Update Non-transaction Read Non-transaction Update Conceptually, when a transaction is initiated an instantaneous snapshot is taken of all KVSs in the specified KVDB for which transactions are enabled. The transaction may then be used to read or update KV pairs in these KVS snapshots. Snapshot isolation is enforced by failing update operations in a transaction that collide with updates in concurrent transactions, after which the transaction may be aborted and retried. In rare cases, the collision detection mechanism may produce false positives. HSE implements asynchronous (non-durable) transaction commits. Committed transactions are made durable via one of several durability controls . Cursors Cursors are used to iterate over keys in a KVS snapshot. A cursor can iterate over keys in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys in a KVS snapshot whose initial bytes match the filter. Tip Cursors deliver significantly greater performance when used with a KVS storing segmented keys, and where a filter is specified with a length equal to or greater than the key prefix length for that KVS. To the degree practical, you should structure applications to avoid using cursors outside of this use case. Furthermore, you should always use get operations instead of cursor seeks when iteration is not required. A cursor can be used to seek to the first key in the cursor's view that is lexicographically equal to or greater than a specified key. The interaction of cursor filters and seek is best described by example. Consider a KVS storing the following keys, which are listed in lexicographic order: \"ab001\", \"af001\", \"af002\", \"ap001\". If a cursor is created for the KVS with a filter of \"af\", then the cursor's view is limited to the keys: \"af001\", \"af002\". If that cursor is then used to seek to the key \"ab\", it will be positioned at the first key in its view equal to or greater than \"ab\", which is \"af001\". Iterating (reading) with the cursor will return the key \"af001\", then \"af002\", and then the EOF condition indicating there are no more keys in view. If instead the cursor is used to seek to the key \"ap\", it will be positioned past the last key in its view, such that an attempt to iterate (read) with the cursor will indicate an EOF condition. There are two types of cursors: non-transaction and transaction. A non-transaction cursor iterates over a KVS snapshot that is taken at the time the cursor is created. However, the cursor's view may be explicitly updated to the latest snapshot of the KVS at any time. A non-transaction cursor can be created for a KVS independent of whether the KVS was opened with transactions enabled or disabled. A transaction cursor iterates over a KVS snapshot associated with an active transaction, including any updates made in that transaction. If the transaction commits or aborts before the cursor is destroyed, the cursor's view reverts to the KVS snapshot taken at the time the transaction first became active. I.e., updates made in the transaction are no longer in the cursor's view. By definition a transaction cursor can only be created for a KVS opened with transactions enabled. A transaction cursor's view cannot be explicitly updated. Durability Controls HSE provides the hse_kvdb_sync() API call to flush cached KVDB updates to stable storage, either synchronously or asynchronously. All cached updates are flushed, whether from non-transaction operations or committed transactions. In the normal case where journaling is enabled ( dur_enable ), cached updates are written to the journal on stable storage. Otherwise, cached updates are written directly to a KVDB media class on stable storage. HSE also supports automatically flushing cached KVDB updates to the journal on stable storage. The frequency for automatically flushing cached updates is controlled by the durability interval ( dur_intvl_ms ) configured for a KVDB. Multithreading HSE supports highly-concurrent multithreaded applications, and most functions in the HSE API are thread-safe. However, there are a few exceptions, as documented in the API reference . Delete Semantics Delete operations logically remove KV pairs from a KVS. However, HSE implements physical removal as a background operation, and hence capacity is not freed immediately.","title":"Concepts"},{"location":"dev/concepts/#concepts","text":"The following describes HSE concepts that are important to understand for developing HSE applications and making effective use of the HSE API.","title":"Concepts"},{"location":"dev/concepts/#kvdb-and-kvs","text":"HSE provides functions to create and access a key-value database (KVDB). A KVDB comprises one or more named key-value stores (KVS), each of which is an independent collection of key-value (KV) pairs. A KVS is analogous to a table in a relational database. HSE provides the standard KV operators for managing KV pairs stored in a KVS: put, get, and delete. HSE also provides transactions, cursors, prefix deletes, and other advanced features. The HSE data model enables each KVS in a KVDB to be optimized for how the KV pairs it stores will be accessed.","title":"KVDB and KVS"},{"location":"dev/concepts/#data-model","text":"Understanding the HSE data model is fundamental to achieving maximum application performance. Adhering to the best practices of this data model can result in significantly greater performance than might be achieved otherwise. While this data model is simple, it has proven very effective.","title":"Data Model"},{"location":"dev/concepts/#key-structure","text":"To describe the HSE data model, we define the following terms. key \u2014 a byte string used to uniquely identify a value for storage, retrieval, and deletion in a KVS segmented key \u2014 a key that is logically divided into N segments, N >= 2 , arranged to group related KV pairs when keys are sorted lexicographically unsegmented key \u2014 a key not logically divided into segments For segmented keys, we further define the following. key prefix \u2014 the first K segments, 1 <= K < N , that group related KV pairs when keys are sorted lexicographically key prefix length \u2014 the length of a key prefix in bytes","title":"Key Structure"},{"location":"dev/concepts/#kvs-configuration","text":"In the common case where related sets of KV pairs are accessed together, best performance is generally achieved by: defining a segmented key with a key prefix such that KV pairs to be accessed together are grouped (contiguous) when keys are sorted lexicographically, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) equal to the key prefix length of the segmented key. In the case where there is no relationship between KV pairs, best performance is generally achieved by: defining an unsegmented key, and creating a KVS to store these KV pairs with a key prefix length parameter ( pfx_len ) of zero (0). Keep in mind that a KVDB may contain multiple KVSs. So in the case where there are multiple collections of KV pairs, each collection can be stored in a different KVS with a key structure, and corresponding KVS pfx_len parameter, that is appropriate for that collection. This is a powerful capability that enables HSE to optimize storage and access for all KV pairs in a KVDB without compromise.","title":"KVS Configuration"},{"location":"dev/concepts/#operation-support","text":"HSE provides several advanced operations with features that directly support the HSE data model. These operations are described in detail later, but we briefly discuss how they support the data model here. Cursors are used to iterate over keys in a KVS in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys whose initial bytes match the filter. The primary use for cursors is with a KVS storing segmented keys, where the length of the specified filter is equal to or greater than the key prefix length for that KVS. Used this way, cursors provide efficient iteration over sets of related KV pairs. Prefix deletes are used to atomically remove all KV pairs in a KVS with keys whose initial bytes match a specified filter. The length of the filter must be equal to the pfx_len parameter of the KVS. The primary use for prefix deletes is with a KVS storing segmented keys. This is a powerful capability that enables sets of related KV pairs to be deleted from a KVS in a single operation without cursor iteration. Transactions are used to execute a sequence of KV operations atomically. Transactions support operating on KV pairs in one or more KVSs in a KVDB. This allows storing multiple collections of KV pairs in different KVSs to optimize access, without giving up the ability to operate on any of those KV pairs within the context of a single transaction.","title":"Operation Support"},{"location":"dev/concepts/#modeling-examples","text":"Below we present examples of applying the HSE data model to the real-world problem of storing and analyzing machine-generated data. Specifically, log data captured from datacenter servers. System logs are commonly retrieved from datacenter servers on a periodic basis and stored for both real-time and historical analysis. We examine several ways this data might be modeled, depending on how it will be accessed and managed.","title":"Modeling Examples"},{"location":"dev/concepts/#simple-log-storage","text":"We start with a simple data model for storing log data in a KVDB. This model uses a single KVS with segmented keys as defined below, and values which are individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 ts Timestamp 16 2 typeID Log type identifier In this and later examples, segment names are for convenience of presentation (they do not exist in the HSE API), and segment lengths are representative. Also, partial or complete keys may be represented as tuples using segment names. For example, (sysId) , (sysID, ts) , or (sysID, ts, typeID) . We define the key prefix to be (sysID) , yielding a key prefix length of 8 bytes. Hence, we would create a KVS for storing these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (sysID) can be used to efficiently iterate over the log records associated with the system sysID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than a (sysID, ts) , and then iterate from there. This data model makes it easy and efficient to search the log records associated with system sysID over an arbitrary time span. However, pruning those log records, for example to retain only those from the past 30 days, requires iterating over all older records and deleting them individually. Next we will look at enhancing this data model to make log record maintenance more efficient.","title":"Simple Log Storage"},{"location":"dev/concepts/#per-system-epoch-based-log-storage","text":"We extend the simple data model from above to include an epoch identifier representing a well-defined time interval. For example, an epoch might be four hours in length, with 6 epochs per day, 42 epochs per week, and so forth. We assume that a log record's timestamp can be mapped to a specific epoch through simple computation. We again use a single KVS but with segmented keys as defined below, and values which are individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 sysID System identifier 8 8 epochID Epoch identifier 16 8 ts Timestamp 24 2 typeID Log type identifier We define the key prefix to be (sysID, epochID) , yielding a key prefix length of 16 bytes. Hence, we would create a KVS for storing these KV pairs with a key prefix length parameter ( pfx_len ) of 16 bytes. With this data model and KVS configuration, a cursor with the filter (sysID, epochID) can be used to efficiently iterate over the log records associated with the system sysID within the epoch epochID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (sysID, epochID, ts) , and then iterate from there within the epoch. With this revised data model, it is still easy and efficient to search the log records associated with system sysID over an arbitrary time span, though it is necessary to configure a cursor to iterate over each epoch of interest (e.g., to cross epoch boundaries). However, we can now prune log records very efficiently by using a single prefix delete to remove all KV pairs with a specified key prefix of (sysID, epochID) . Next we examine a variation on this per-system epoch-based data model.","title":"Per-System Epoch-based Log Storage"},{"location":"dev/concepts/#all-systems-epoch-based-log-storage","text":"The previous data model makes it easy and efficient to iterate over the log records associated with a given sysID for a specified epochID . However, to view records from multiple systems for a specified epochID requires a cursor iteration per sysID of interest. We can instead group the log records for all systems within an epoch by using a KVS with segmented keys as defined below, and values which are individual log records. Key offset Segment Length (bytes) Segment Name Description 0 8 epochID Epoch identifier 8 8 ts Timestamp 16 8 sysID System identifier 24 2 typeID Log type identifier We define the key prefix to be (epochID) , yielding a key prefix length of 8 bytes. Hence we would create a KVS for storing these KV pairs with a key prefix length parameter ( pfx_len ) of 8 bytes. With this data model and KVS configuration, a cursor with the filter (epochID) can be used to efficiently iterate over the log records for all systems within the epoch epochID in timestamp order. Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (epochID, ts) , and then iterate from there within the epoch. It should be clear that there is some loss of efficiency, versus the prior data model, to obtain all the log records associated with a specified sysID over an arbitrary time span, because records associated with systems other than sysID will be in the cursor's view and must be skipped. However, this is still a reasonably efficient query because it meets the criteria that the length of the cursor filter be equal to or greater than the key prefix length to achieve maximum performance. With this revised data model, pruning log records remains efficient because we can still use a single prefix delete to remove all KV pairs with a specified key prefix of (epochID) . In this case, the records for all systems in the epoch are pruned together. Tip This particular data model provides the opportunity to point out another best practice that can result in significantly greater performance. For a KVS storing segmented keys, it is important that the key prefix specified in put operations takes on a modest number of different values over a consecutive sequence of puts. For this example, that means choosing an epoch that is relatively short versus what one might select with the prior data model. Finally, we will examine an index-based data model for log storage.","title":"All-Systems Epoch-based Log Storage"},{"location":"dev/concepts/#index-based-log-storage","text":"The prior data models for log storage have the benefit of simplicity in that the KVDB has only a single KVS. However, they may provide less flexibility than required by the application. In this next example, we demonstrate how to use multiple KVSs to effectively index log records. For brevity, we define the key structure for each KVS using the tuple syntax, segment names, and segment lengths adopted in prior examples. KVS Name Key Type Key Key Prefix KVS pfx_len Value logRec segmented (epochID, ts, sysID, typeID) (epochID) 8 Log record content sysIdx segmented (sysID, epochID, ts, typeID) (sysID, epochID) 16 The KVS logRec stores the content of all log records, with each log record uniquely identified by the segmented key (epochID, ts, sysID, typeID) with key prefix (epochID) . Hence we would create KVS logRec with a key prefix length parameter ( pfx_len ) of 8 bytes. The KVS sysIdx is an index for the log records stored in KVS logRec , with a key pefix of (sysID, epochID) . Hence we would create KVS sysIdx with a key prefix length parameter ( pfx_len ) of 16 bytes. Using KVS logRec , a cursor with the filter (epochID) can be used to efficiently iterate over the log records for all systems within the epoch epochID in timestamp order. Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (epochID, ts) , and then iterate from there within the epoch. Using KVS sysIdx , a cursor with the filter (sysID, epochID) can be used to efficiently iterate over the log records associated with the system sysID within the epoch epochID . Furthermore, the cursor can be used to efficiently seek to the first key (in the cursor's view) that is lexicographically equal to or greater than (sysID, epochID, ts) , and then iterate from there within the epoch. Note that KVS sysIdx does not contain any log record contents, and in fact stores no values at all (i.e., all value lengths are zero). To obtain the contents of a specific log record, there is the added step of a get operation in KVS logRec with key (epochID, ts, sysID, typeID) . The above schema accomplishes the following. Efficient iteration over the log records from all systems in a specified epoch using a cursor with KVS logRec . Efficient iteration over the log records from a specific system in a specified epoch using a cursor with KVS sysIdx . No duplication of log record content. A transaction can be used to atomically put (insert) KV pairs for a given log record in KVS logRec and KVS sysIdx . This guarantees integrity of the sysIdx index. Pruning log records is a bit more complex than the prior data models, bit still relatively straight-forward. To delete all log records for a given epochID , where the epoch is assumed to have passed, you would use a cursor with filter (epochID) to iterate over KVS logRec to build a list of all sysID in the epoch, and then in a single transaction, prefix delete each key prefix (sysID, epochID) from KVS sysIdx , and prefix delete the key prefix (epochID) from KVS logRec .","title":"Index-based Log Storage"},{"location":"dev/concepts/#snapshots","text":"HSE uses multiversion concurrency control ( MVCC ) techniques to implement industry-standard snapshot isolation semantics for transactions and cursors. In this model, transactions and cursors operate on KVS snapshots in a KVDB. Conceptually, a KVS snapshot contains KV pairs from all transactions committed, or non-transaction operations completed, at the time the KVS snapshot is taken. A KVS snapshot is ephemeral and ceases to exist when all associated transaction and cursor operations complete.","title":"Snapshots"},{"location":"dev/concepts/#transactions","text":"Transactions are used to execute a sequence of KV operations as a unit of work that is atomic, consistent, isolated, and durable ( ACID ). A transaction may operate on KV pairs in one or more KVSs in a KVDB. When a KVS is opened, the transactions_enable parameter specifies whether or not that KVS supports transactions. This is not a persistent setting in that the KVS may be closed and later reopened in a different mode. The following table specifies the operations that may be performed on a KVS opened with transactions enabled or disabled, where: Read is a query operation, such as get or cursor iteration Update is a mutation operation, such as put, delete, or prefix delete KVS Transactions Enabled KVS Transactions Disabled Transaction Read Transaction Update Non-transaction Read Non-transaction Update Conceptually, when a transaction is initiated an instantaneous snapshot is taken of all KVSs in the specified KVDB for which transactions are enabled. The transaction may then be used to read or update KV pairs in these KVS snapshots. Snapshot isolation is enforced by failing update operations in a transaction that collide with updates in concurrent transactions, after which the transaction may be aborted and retried. In rare cases, the collision detection mechanism may produce false positives. HSE implements asynchronous (non-durable) transaction commits. Committed transactions are made durable via one of several durability controls .","title":"Transactions"},{"location":"dev/concepts/#cursors","text":"Cursors are used to iterate over keys in a KVS snapshot. A cursor can iterate over keys in forward or reverse lexicographic order. Cursors support an optional filter , which is a byte string limiting a cursor's view to only those keys in a KVS snapshot whose initial bytes match the filter. Tip Cursors deliver significantly greater performance when used with a KVS storing segmented keys, and where a filter is specified with a length equal to or greater than the key prefix length for that KVS. To the degree practical, you should structure applications to avoid using cursors outside of this use case. Furthermore, you should always use get operations instead of cursor seeks when iteration is not required. A cursor can be used to seek to the first key in the cursor's view that is lexicographically equal to or greater than a specified key. The interaction of cursor filters and seek is best described by example. Consider a KVS storing the following keys, which are listed in lexicographic order: \"ab001\", \"af001\", \"af002\", \"ap001\". If a cursor is created for the KVS with a filter of \"af\", then the cursor's view is limited to the keys: \"af001\", \"af002\". If that cursor is then used to seek to the key \"ab\", it will be positioned at the first key in its view equal to or greater than \"ab\", which is \"af001\". Iterating (reading) with the cursor will return the key \"af001\", then \"af002\", and then the EOF condition indicating there are no more keys in view. If instead the cursor is used to seek to the key \"ap\", it will be positioned past the last key in its view, such that an attempt to iterate (read) with the cursor will indicate an EOF condition. There are two types of cursors: non-transaction and transaction. A non-transaction cursor iterates over a KVS snapshot that is taken at the time the cursor is created. However, the cursor's view may be explicitly updated to the latest snapshot of the KVS at any time. A non-transaction cursor can be created for a KVS independent of whether the KVS was opened with transactions enabled or disabled. A transaction cursor iterates over a KVS snapshot associated with an active transaction, including any updates made in that transaction. If the transaction commits or aborts before the cursor is destroyed, the cursor's view reverts to the KVS snapshot taken at the time the transaction first became active. I.e., updates made in the transaction are no longer in the cursor's view. By definition a transaction cursor can only be created for a KVS opened with transactions enabled. A transaction cursor's view cannot be explicitly updated.","title":"Cursors"},{"location":"dev/concepts/#durability-controls","text":"HSE provides the hse_kvdb_sync() API call to flush cached KVDB updates to stable storage, either synchronously or asynchronously. All cached updates are flushed, whether from non-transaction operations or committed transactions. In the normal case where journaling is enabled ( dur_enable ), cached updates are written to the journal on stable storage. Otherwise, cached updates are written directly to a KVDB media class on stable storage. HSE also supports automatically flushing cached KVDB updates to the journal on stable storage. The frequency for automatically flushing cached updates is controlled by the durability interval ( dur_intvl_ms ) configured for a KVDB.","title":"Durability Controls"},{"location":"dev/concepts/#multithreading","text":"HSE supports highly-concurrent multithreaded applications, and most functions in the HSE API are thread-safe. However, there are a few exceptions, as documented in the API reference .","title":"Multithreading"},{"location":"dev/concepts/#delete-semantics","text":"Delete operations logically remove KV pairs from a KVS. However, HSE implements physical removal as a background operation, and hence capacity is not freed immediately.","title":"Delete Semantics"},{"location":"dev/limits/","text":"Limits The tables below provide guidance on HSE operating limits. A few are enforced, but most are based on testing and experience. The limits appropriate for a specific HSE application are largely dependent on the performance requirements of that application, and the hardware it runs on. Feel free to push these limits in testing and let us know how far you get and what you observe. KVDB Limits Entity Description Limit Enforced KVDB count Active KVDB per system 8 No KVS count KVSs in a KVDB 16 No Key count Total keys in a KVDB (billions) 200 No Capacity Total storage capacity of a KVDB (TB) 12 No Transaction count Concurrent transactions in a KVDB 1,000 per CPU Yes Cursor count Concurrent cursors in a KVDB 10,000 No Info An application can only have a single KVDB open at a time. We expect to remove this limitation in a later release. KVS Limits Entity Description Limit Enforced Key size Range of valid key sizes (bytes) 1 \u2013 1,334 Yes Value size Range of valid value sizes (bytes) 0 \u2013 1MiB Yes Key count Total keys in a KVS (billions) 50 No Capacity Total storage capacity of a KVS (TB) 4 No Cursor count Concurrent cursors in a KVS 8,000 No","title":"Limits"},{"location":"dev/limits/#limits","text":"The tables below provide guidance on HSE operating limits. A few are enforced, but most are based on testing and experience. The limits appropriate for a specific HSE application are largely dependent on the performance requirements of that application, and the hardware it runs on. Feel free to push these limits in testing and let us know how far you get and what you observe.","title":"Limits"},{"location":"dev/limits/#kvdb-limits","text":"Entity Description Limit Enforced KVDB count Active KVDB per system 8 No KVS count KVSs in a KVDB 16 No Key count Total keys in a KVDB (billions) 200 No Capacity Total storage capacity of a KVDB (TB) 12 No Transaction count Concurrent transactions in a KVDB 1,000 per CPU Yes Cursor count Concurrent cursors in a KVDB 10,000 No Info An application can only have a single KVDB open at a time. We expect to remove this limitation in a later release.","title":"KVDB Limits"},{"location":"dev/limits/#kvs-limits","text":"Entity Description Limit Enforced Key size Range of valid key sizes (bytes) 1 \u2013 1,334 Yes Value size Range of valid value sizes (bytes) 0 \u2013 1MiB Yes Key count Total keys in a KVS (billions) 50 No Capacity Total storage capacity of a KVS (TB) 4 No Cursor count Concurrent cursors in a KVS 8,000 No","title":"KVS Limits"},{"location":"gs/about/","text":"About HSE is a fast embeddable key-value store designed for SSDs and persistent memory. It is implemented as a C library that links with your application, directly or via alternate language bindings. Overview HSE provides functions to create and access a key-value database (KVDB). A KVDB comprises one or more named key-value stores (KVS), each of which is an independent collection of key-value pairs. A KVS is analogous to a table in a relational database. The HSE data model enables each KVS in a KVDB to be optimized for how the key-value pairs it stores will be accessed. HSE provides the standard operators for managing key-value pairs in a KVS: put, get, and delete. HSE also provides transactions, cursors, prefix deletes, and other advanced features. Details on the HSE programming and data models can be found here . Details on how KVDB data is stored in file systems can be found here . Project Repos The HSE project includes the following repos: hse contains the HSE library source hse-python contains HSE Python bindings hse-mongo is a fork that integrates HSE with MongoDB hse-ycsb is a fork that integrates HSE with the YCSB benchmark rfcs contains RFCs for major enhancements to the HSE library or other project components hse-project.github.io contains the source for this documentation Instructions for building and installing repo contents are in their local README.md files (or README_HSE.md for forked repos). After building and installing the hse repo you can develop HSE applications by including the hse.h header file and linking with libhse-2 .","title":"About"},{"location":"gs/about/#about","text":"HSE is a fast embeddable key-value store designed for SSDs and persistent memory. It is implemented as a C library that links with your application, directly or via alternate language bindings.","title":"About"},{"location":"gs/about/#overview","text":"HSE provides functions to create and access a key-value database (KVDB). A KVDB comprises one or more named key-value stores (KVS), each of which is an independent collection of key-value pairs. A KVS is analogous to a table in a relational database. The HSE data model enables each KVS in a KVDB to be optimized for how the key-value pairs it stores will be accessed. HSE provides the standard operators for managing key-value pairs in a KVS: put, get, and delete. HSE also provides transactions, cursors, prefix deletes, and other advanced features. Details on the HSE programming and data models can be found here . Details on how KVDB data is stored in file systems can be found here .","title":"Overview"},{"location":"gs/about/#project-repos","text":"The HSE project includes the following repos: hse contains the HSE library source hse-python contains HSE Python bindings hse-mongo is a fork that integrates HSE with MongoDB hse-ycsb is a fork that integrates HSE with the YCSB benchmark rfcs contains RFCs for major enhancements to the HSE library or other project components hse-project.github.io contains the source for this documentation Instructions for building and installing repo contents are in their local README.md files (or README_HSE.md for forked repos). After building and installing the hse repo you can develop HSE applications by including the hse.h header file and linking with libhse-2 .","title":"Project Repos"},{"location":"gs/cli/","text":"Command Line Interface Applications will normally use the HSE API to create and manage a KVDB. However, the HSE command line interface (CLI) can also be used for these tasks. The CLI is itself an HSE application. Below are several usage examples which assume the following. /var/bulk is a file system on storage suitable for a capacity media class /var/fast is a file system on storage suitable for a staging media class See the discussion on KVDB storage for more details on media classes. Create a KVDB Create a KVDB taking all the defaults. mkdir /var/bulk/kvdb1 && cd /var/bulk/kvdb1 hse kvdb create The KVDB home defaults to the current working directory /var/bulk/kvdb1 , and the required capacity media class directory defaults to /var/bulk/kvdb1/capacity . Next create a KVDB specifying the home directory. mkdir /var/bulk/kvdb2 hse -C /var/bulk/kvdb2 kvdb create The specified KVDB home directory is /var/bulk/kvdb2 , and the required capacity media class directory defaults to /var/bulk/kvdb2/capacity . Finally, create a KVDB specifying the home directory and KVDB parameters for both the required capacity and optional staging media class directories. mkdir /var/bulk/kvdb3 && mkdir /var/bulk/capacity3 && mkdir /var/fast/staging3 hse -C /var/bulk/kvdb3 kvdb create storage.capacity.path = /var/bulk/capacity3 storage.staging.path = /var/fast/staging3 The specified KVDB home directory is /var/bulk/kvdb3 , the capacity media class directory is /var/bulk/capacity3 , and the staging media class directory is /var/fast/staging3 . Create a KVS Create a KVS in a KVDB taking all the defaults. cd /var/bulk/kvdb1 hse kvs create kvs1 The KVDB home defaults to the current working directory /var/bulk/kvdb1 , and the KVS named kvs1 is created there with the default key prefix length ( pfx_len ) of zero (0). Next create a KVS specifying the KVDB home directory and KVS key prefix length. hse -C /var/bulk/kvdb2 kvs create kvs1 pfx_len = 8 The specified KVDB home directory is /var/bulk/kvdb2 , and the KVS named kvs1 is created there with a key prefix length of 8 bytes. Get KVDB Information Get information about a KVDB. hse -C /var/bulk/kvdb1 kvdb info This command will print out information about a KVDB, including its home directory, KVS list, and storage space metrics. These storage space metrics include: Total space in the file systems hosting the KVDB media classes Available space in the file systems hosting the KVDB media classes Space allocated for the KVDB Space used by the KVDB, which is always less than or equal to the allocated space Compact a KVDB HSE implements the physical removal of logically deleted data via a background operation called compaction. Though generally not required, compaction can be initiated manually. Be aware that compaction can take several minutes or longer depending on the amount of data stored in the KVDB, among many other factors, so a timeout can be specified in seconds. cd /var/bulk/kvdb1 hse kvdb compact --timeout 120 If an application has the KVDB open, the compaction may continue past the timeout value. In this case, the status of the compaction can be queried. hse kvdb compact --status A compaction operation can also be canceled. hse kvdb compact --cancel Drop a KVS Drop (delete) a KVS in a KVDB. hse -C /var/bulk/kvdb2 kvs drop kvs1 The specified KVDB home directory is /var/bulk/kvdb2 , and the KVS named kvs1 is dropped from there. Drop a KVDB Drop (delete) a KVDB and all of its KVSs. cd /var/bulk/kvdb3 hse kvdb drop The KVDB home defaults to the current working directory /var/bulk/kvdb3 , and that KVDB is dropped. Next drop the remaining KVDBs from these examples specifying their home directories. hse -C /var/bulk/kvdb2 kvdb drop hse -C /var/bulk/kvdb1 kvdb drop","title":"Command Line Interface"},{"location":"gs/cli/#command-line-interface","text":"Applications will normally use the HSE API to create and manage a KVDB. However, the HSE command line interface (CLI) can also be used for these tasks. The CLI is itself an HSE application. Below are several usage examples which assume the following. /var/bulk is a file system on storage suitable for a capacity media class /var/fast is a file system on storage suitable for a staging media class See the discussion on KVDB storage for more details on media classes.","title":"Command Line Interface"},{"location":"gs/cli/#create-a-kvdb","text":"Create a KVDB taking all the defaults. mkdir /var/bulk/kvdb1 && cd /var/bulk/kvdb1 hse kvdb create The KVDB home defaults to the current working directory /var/bulk/kvdb1 , and the required capacity media class directory defaults to /var/bulk/kvdb1/capacity . Next create a KVDB specifying the home directory. mkdir /var/bulk/kvdb2 hse -C /var/bulk/kvdb2 kvdb create The specified KVDB home directory is /var/bulk/kvdb2 , and the required capacity media class directory defaults to /var/bulk/kvdb2/capacity . Finally, create a KVDB specifying the home directory and KVDB parameters for both the required capacity and optional staging media class directories. mkdir /var/bulk/kvdb3 && mkdir /var/bulk/capacity3 && mkdir /var/fast/staging3 hse -C /var/bulk/kvdb3 kvdb create storage.capacity.path = /var/bulk/capacity3 storage.staging.path = /var/fast/staging3 The specified KVDB home directory is /var/bulk/kvdb3 , the capacity media class directory is /var/bulk/capacity3 , and the staging media class directory is /var/fast/staging3 .","title":"Create a KVDB"},{"location":"gs/cli/#create-a-kvs","text":"Create a KVS in a KVDB taking all the defaults. cd /var/bulk/kvdb1 hse kvs create kvs1 The KVDB home defaults to the current working directory /var/bulk/kvdb1 , and the KVS named kvs1 is created there with the default key prefix length ( pfx_len ) of zero (0). Next create a KVS specifying the KVDB home directory and KVS key prefix length. hse -C /var/bulk/kvdb2 kvs create kvs1 pfx_len = 8 The specified KVDB home directory is /var/bulk/kvdb2 , and the KVS named kvs1 is created there with a key prefix length of 8 bytes.","title":"Create a KVS"},{"location":"gs/cli/#get-kvdb-information","text":"Get information about a KVDB. hse -C /var/bulk/kvdb1 kvdb info This command will print out information about a KVDB, including its home directory, KVS list, and storage space metrics. These storage space metrics include: Total space in the file systems hosting the KVDB media classes Available space in the file systems hosting the KVDB media classes Space allocated for the KVDB Space used by the KVDB, which is always less than or equal to the allocated space","title":"Get KVDB Information"},{"location":"gs/cli/#compact-a-kvdb","text":"HSE implements the physical removal of logically deleted data via a background operation called compaction. Though generally not required, compaction can be initiated manually. Be aware that compaction can take several minutes or longer depending on the amount of data stored in the KVDB, among many other factors, so a timeout can be specified in seconds. cd /var/bulk/kvdb1 hse kvdb compact --timeout 120 If an application has the KVDB open, the compaction may continue past the timeout value. In this case, the status of the compaction can be queried. hse kvdb compact --status A compaction operation can also be canceled. hse kvdb compact --cancel","title":"Compact a KVDB"},{"location":"gs/cli/#drop-a-kvs","text":"Drop (delete) a KVS in a KVDB. hse -C /var/bulk/kvdb2 kvs drop kvs1 The specified KVDB home directory is /var/bulk/kvdb2 , and the KVS named kvs1 is dropped from there.","title":"Drop a KVS"},{"location":"gs/cli/#drop-a-kvdb","text":"Drop (delete) a KVDB and all of its KVSs. cd /var/bulk/kvdb3 hse kvdb drop The KVDB home defaults to the current working directory /var/bulk/kvdb3 , and that KVDB is dropped. Next drop the remaining KVDBs from these examples specifying their home directories. hse -C /var/bulk/kvdb2 kvdb drop hse -C /var/bulk/kvdb1 kvdb drop","title":"Drop a KVDB"},{"location":"gs/params/","text":"Configuration Parameters HSE defines global, KVDB, and KVS configuration parameters as described below. These are classified as either create-time parameters, which apply when an application creates a KVDB or KVS, or runtime parameters, which apply each time an application initializes the HSE library or opens a KVDB or KVS. Global Parameters Global parameters are runtime parameters that apply at the application level. These parameters may be specified in the hse_init() API call or in the optional hse.conf JSON file in the runtime home directory, which is also specified in hse_init() . For hse_init() API calls, specify global parameters in the form <param>=<value> . For example, socket.enabled=false . The following global parameters are part of the stable API. Parameter Default Description logging.enabled true Logging mode (false==disabled, true==enabled) logging.structured true Logging style (false==basic, true==structured) logging.destination syslog Log destination (stdout, stderr, file, syslog) logging.path <runtime home>/hse.log Log file when logging.destination==file logging.level 7 Logging severity level (0==emergency; 7==debug) socket.enabled true REST interface mode (false==disabled, true==enabled) socket.path /tmp/hse-<pid>.sock UNIX domain socket file when socket.enabled==true KVDB Parameters KVDB parameters apply when a KVDB is created or opened. KVDB Create-time Parameters KVDB create-time parameters may be specified in the hse_kvdb_create() API call or when using the CLI to create a KVDB. In either case, specify KVDB create-time parameters in the form <param>=<value> . For example, storage.capacity.path=/path/to/capacity/dir . The following KVDB create-time parameters are part of the stable API. Parameter Default Description storage.capacity.path <KVDB home>/capacity Capacity media class directory storage.staging.path Staging media class directory KVDB Runtime Parameters KVDB runtime parameters may be specified in the hse_kvdb_open() API call or in the optional kvdb.conf JSON file in the KVDB home directory, which is also specified in hse_kvdb_open() . For hse_kvdb_open() API calls, specify KVDB runtime parameters in the form <param>=<value> . For example, dur_intvl_ms=1000 . The following KVDB runtime parameters are part of the stable API. Parameter Default Description read_only false Access mode (false==read/write, true==read-only) dur_enable true Journaling mode (false==disabled, true==enabled) dur_intvl_ms 500 Max time data is cached (milliseconds) when dur_enable==true dur_mclass capacity Media class for journal (capacity, staging) throttle_init_policy default Ingest throttle at startup (light, medium, default) Durability Settings The KVDB durability parameters control how HSE journals updates for that KVDB to provide for recovery in the event of a failure. The parameter dur_enable determines whether or not journaling is enabled. In general, you should always set this to true . As a rare exception, applications that implement their own form of durability may want to disable HSE journaling to increase performance. The parameter dur_intvl_ms specifies the frequency (in milliseconds) for automatically flushing cached updates to the journal on stable storage. Increasing this value may improve performance but also increases the amount of data that may be lost in the event of a failure. The parameter dur_mclass specifies the media class for storing journal files. In general, best performance is achieved by storing the journal files on the fastest media class configured for a KVDB. See the discussion on HSE durability controls for additional details. Initial Throttle Setting On startup, HSE throttles the rate at which it processes updates in a KVDB, referred to as the ingest rate . HSE increases the ingest rate for the KVDB until it reaches the maximum sustainable value for the underlying storage. This ramp-up process can take up to 200 seconds . For benchmarks, this initial throttling can greatly distort results. In general operation, this initial throttling may impact the time before a service is fully operational. The throttle_init_policy parameter can be used to achieve the maximum ingest rate in far less time. It specifies a relative initial throttling value of light (minimum), medium , or default (maximum) throttling. Setting the throttle_init_policy parameter improperly for the underlying storage can cause the durability interval ( dur_intvl_ms ) to be violated or internal indexing structures to become unbalanced for a period of time. For example, this may occur if throttle_init_policy is set to light with relatively slow KVDB storage. The kvdb_profile tool is provided to determine an appropriate throttle_init_policy setting for your KVDB storage. You can run it as follows. kvdb_profile /path/to/capacity/storage/for/the/kvdb The path specified in kvdb_profile should be a directory in the file system hosting the capacity media class for the KVDB of interest. Use the output of kvdb_profile to specify the throttle_init_policy value for that KVDB. KVS Parameters KVS parameters apply when a KVS is created or opened. KVS Create-time Parameters KVS create-time parameters may be specified in the hse_kvdb_kvs_create() API call or when using the CLI to create a KVS. In either case, specify KVS create-time parameters in the form <param>=<value> . For example, pfx_len=8 . The following KVS create-time parameters are part of the stable API. Parameter Default Description pfx_len 0 Key prefix length (bytes) Tip The KVS name default is reserved and may not be used in hse_kvdb_kvs_create() API calls or with the CLI. KVS Runtime Parameters KVS runtime parameters may be specified in the hse_kvdb_kvs_open() API call or in the optional kvdb.conf JSON file in the KVDB home directory, which is also specified in hse_kvdb_open() . For hse_kvdb_kvs_open() API calls, specify KVS runtime parameters in the form <param>=<value> . For example, value_compression=lz4 . The following KVS runtime parameters are part of the stable API. Parameter Default Description transactions_enable false Transaction mode (false==disabled, true==enabled) mclass_policy capacity_only See discussion below for values value_compression none Value compression method (none, lz4) vcompmin 12 Value length above which compression is attempted (bytes) Transaction Mode When a KVS is opened, the transactions_enable value determines whether or not transactions are enabled for the KVS. This mode may be changed by closing and reopening the KVS. See the discussion on HSE transactions for additional details. Media Class Usage The media class usage policy for a KVS defines how the key-value data in that KVS is stored and managed in a KVDB configured with a staging media class. Key-value data in a KVS can either be pinned to a particular media class, or tiered from the staging media class to the capacity media class as it ages, as determined by the mclass_policy value for the KVS: capacity_only pins all key-value data to the capacity media class staging_only pins all key-value data to the staging media class staging_min_capacity tiers all key-value data staging_max_capacity pins all key data to the staging media class and tiers all value data Of the two tiering options, staging_max_capacity will generally yield the highest throughput, lowest latency, and least write-amplification in the capacity media class. The trade-off is more storage required in the staging media class. Info If no staging media class is present, and an mclass_policy value other than capacity_only is specified, a warning is logged and capacity_only is applied. Configuration Files The following are the JSON file formats for the optional HSE configuration files. hse.conf JSON File See the discussion on global parameters for definitions, legal values, and defaults. { \"logging\": { \"enabled\": boolean, \"structured\": boolean, \"destination\": \"stdout | stderr | file | syslog\", \"path\": \"/log/file/path\", \"level\": integer }, \"socket\": { \"enabled\": boolean, \"path\": \"/UNIX/socket/file/path\" } } kvdb.conf JSON File See the discussions on KVDB runtime parameters and KVS runtime parameters for definitions, legal values, and defaults. { \"read_only\": boolean, \"dur_enable\": boolean, \"dur_intvl_ms\": integer, \"dur_mclass\": \"capacity | staging\", \"throttle_init_policy\": \"light | medium | default\", \"kvs\": { \"<kvs name>\": { \"transactions_enable\": boolean, \"mclass_policy\": \"see KVS runtime parameter discussion for value strings\", \"value_compression\": \"lz4 | none\", \"vcompmin\" : integer } } } The KVS name default is reserved and its parameters apply to all the KVS in a KVDB. Parameters specified for a named KVS override those specified via default . Precedence of Parameters The final value for a specific configuration parameter is determined of follows: The built-in default value is applied first The value is then overridden by an API (or CLI) setting, if any The value is then overridden by an hse.conf or kvdb.conf setting, if any This ordering allows the effective value of a configuration parameter to be modified without recompiling an HSE application.","title":"Configuration Parameters"},{"location":"gs/params/#configuration-parameters","text":"HSE defines global, KVDB, and KVS configuration parameters as described below. These are classified as either create-time parameters, which apply when an application creates a KVDB or KVS, or runtime parameters, which apply each time an application initializes the HSE library or opens a KVDB or KVS.","title":"Configuration Parameters"},{"location":"gs/params/#global-parameters","text":"Global parameters are runtime parameters that apply at the application level. These parameters may be specified in the hse_init() API call or in the optional hse.conf JSON file in the runtime home directory, which is also specified in hse_init() . For hse_init() API calls, specify global parameters in the form <param>=<value> . For example, socket.enabled=false . The following global parameters are part of the stable API. Parameter Default Description logging.enabled true Logging mode (false==disabled, true==enabled) logging.structured true Logging style (false==basic, true==structured) logging.destination syslog Log destination (stdout, stderr, file, syslog) logging.path <runtime home>/hse.log Log file when logging.destination==file logging.level 7 Logging severity level (0==emergency; 7==debug) socket.enabled true REST interface mode (false==disabled, true==enabled) socket.path /tmp/hse-<pid>.sock UNIX domain socket file when socket.enabled==true","title":"Global Parameters"},{"location":"gs/params/#kvdb-parameters","text":"KVDB parameters apply when a KVDB is created or opened.","title":"KVDB Parameters"},{"location":"gs/params/#kvdb-create-time-parameters","text":"KVDB create-time parameters may be specified in the hse_kvdb_create() API call or when using the CLI to create a KVDB. In either case, specify KVDB create-time parameters in the form <param>=<value> . For example, storage.capacity.path=/path/to/capacity/dir . The following KVDB create-time parameters are part of the stable API. Parameter Default Description storage.capacity.path <KVDB home>/capacity Capacity media class directory storage.staging.path Staging media class directory","title":"KVDB Create-time Parameters"},{"location":"gs/params/#kvdb-runtime-parameters","text":"KVDB runtime parameters may be specified in the hse_kvdb_open() API call or in the optional kvdb.conf JSON file in the KVDB home directory, which is also specified in hse_kvdb_open() . For hse_kvdb_open() API calls, specify KVDB runtime parameters in the form <param>=<value> . For example, dur_intvl_ms=1000 . The following KVDB runtime parameters are part of the stable API. Parameter Default Description read_only false Access mode (false==read/write, true==read-only) dur_enable true Journaling mode (false==disabled, true==enabled) dur_intvl_ms 500 Max time data is cached (milliseconds) when dur_enable==true dur_mclass capacity Media class for journal (capacity, staging) throttle_init_policy default Ingest throttle at startup (light, medium, default)","title":"KVDB Runtime Parameters"},{"location":"gs/params/#durability-settings","text":"The KVDB durability parameters control how HSE journals updates for that KVDB to provide for recovery in the event of a failure. The parameter dur_enable determines whether or not journaling is enabled. In general, you should always set this to true . As a rare exception, applications that implement their own form of durability may want to disable HSE journaling to increase performance. The parameter dur_intvl_ms specifies the frequency (in milliseconds) for automatically flushing cached updates to the journal on stable storage. Increasing this value may improve performance but also increases the amount of data that may be lost in the event of a failure. The parameter dur_mclass specifies the media class for storing journal files. In general, best performance is achieved by storing the journal files on the fastest media class configured for a KVDB. See the discussion on HSE durability controls for additional details.","title":"Durability Settings"},{"location":"gs/params/#initial-throttle-setting","text":"On startup, HSE throttles the rate at which it processes updates in a KVDB, referred to as the ingest rate . HSE increases the ingest rate for the KVDB until it reaches the maximum sustainable value for the underlying storage. This ramp-up process can take up to 200 seconds . For benchmarks, this initial throttling can greatly distort results. In general operation, this initial throttling may impact the time before a service is fully operational. The throttle_init_policy parameter can be used to achieve the maximum ingest rate in far less time. It specifies a relative initial throttling value of light (minimum), medium , or default (maximum) throttling. Setting the throttle_init_policy parameter improperly for the underlying storage can cause the durability interval ( dur_intvl_ms ) to be violated or internal indexing structures to become unbalanced for a period of time. For example, this may occur if throttle_init_policy is set to light with relatively slow KVDB storage. The kvdb_profile tool is provided to determine an appropriate throttle_init_policy setting for your KVDB storage. You can run it as follows. kvdb_profile /path/to/capacity/storage/for/the/kvdb The path specified in kvdb_profile should be a directory in the file system hosting the capacity media class for the KVDB of interest. Use the output of kvdb_profile to specify the throttle_init_policy value for that KVDB.","title":"Initial Throttle Setting"},{"location":"gs/params/#kvs-parameters","text":"KVS parameters apply when a KVS is created or opened.","title":"KVS Parameters"},{"location":"gs/params/#kvs-create-time-parameters","text":"KVS create-time parameters may be specified in the hse_kvdb_kvs_create() API call or when using the CLI to create a KVS. In either case, specify KVS create-time parameters in the form <param>=<value> . For example, pfx_len=8 . The following KVS create-time parameters are part of the stable API. Parameter Default Description pfx_len 0 Key prefix length (bytes) Tip The KVS name default is reserved and may not be used in hse_kvdb_kvs_create() API calls or with the CLI.","title":"KVS Create-time Parameters"},{"location":"gs/params/#kvs-runtime-parameters","text":"KVS runtime parameters may be specified in the hse_kvdb_kvs_open() API call or in the optional kvdb.conf JSON file in the KVDB home directory, which is also specified in hse_kvdb_open() . For hse_kvdb_kvs_open() API calls, specify KVS runtime parameters in the form <param>=<value> . For example, value_compression=lz4 . The following KVS runtime parameters are part of the stable API. Parameter Default Description transactions_enable false Transaction mode (false==disabled, true==enabled) mclass_policy capacity_only See discussion below for values value_compression none Value compression method (none, lz4) vcompmin 12 Value length above which compression is attempted (bytes)","title":"KVS Runtime Parameters"},{"location":"gs/params/#transaction-mode","text":"When a KVS is opened, the transactions_enable value determines whether or not transactions are enabled for the KVS. This mode may be changed by closing and reopening the KVS. See the discussion on HSE transactions for additional details.","title":"Transaction Mode"},{"location":"gs/params/#media-class-usage","text":"The media class usage policy for a KVS defines how the key-value data in that KVS is stored and managed in a KVDB configured with a staging media class. Key-value data in a KVS can either be pinned to a particular media class, or tiered from the staging media class to the capacity media class as it ages, as determined by the mclass_policy value for the KVS: capacity_only pins all key-value data to the capacity media class staging_only pins all key-value data to the staging media class staging_min_capacity tiers all key-value data staging_max_capacity pins all key data to the staging media class and tiers all value data Of the two tiering options, staging_max_capacity will generally yield the highest throughput, lowest latency, and least write-amplification in the capacity media class. The trade-off is more storage required in the staging media class. Info If no staging media class is present, and an mclass_policy value other than capacity_only is specified, a warning is logged and capacity_only is applied.","title":"Media Class Usage"},{"location":"gs/params/#configuration-files","text":"The following are the JSON file formats for the optional HSE configuration files.","title":"Configuration Files"},{"location":"gs/params/#hseconf-json-file","text":"See the discussion on global parameters for definitions, legal values, and defaults. { \"logging\": { \"enabled\": boolean, \"structured\": boolean, \"destination\": \"stdout | stderr | file | syslog\", \"path\": \"/log/file/path\", \"level\": integer }, \"socket\": { \"enabled\": boolean, \"path\": \"/UNIX/socket/file/path\" } }","title":"hse.conf JSON File"},{"location":"gs/params/#kvdbconf-json-file","text":"See the discussions on KVDB runtime parameters and KVS runtime parameters for definitions, legal values, and defaults. { \"read_only\": boolean, \"dur_enable\": boolean, \"dur_intvl_ms\": integer, \"dur_mclass\": \"capacity | staging\", \"throttle_init_policy\": \"light | medium | default\", \"kvs\": { \"<kvs name>\": { \"transactions_enable\": boolean, \"mclass_policy\": \"see KVS runtime parameter discussion for value strings\", \"value_compression\": \"lz4 | none\", \"vcompmin\" : integer } } } The KVS name default is reserved and its parameters apply to all the KVS in a KVDB. Parameters specified for a named KVS override those specified via default .","title":"kvdb.conf JSON File"},{"location":"gs/params/#precedence-of-parameters","text":"The final value for a specific configuration parameter is determined of follows: The built-in default value is applied first The value is then overridden by an API (or CLI) setting, if any The value is then overridden by an hse.conf or kvdb.conf setting, if any This ordering allows the effective value of a configuration parameter to be modified without recompiling an HSE application.","title":"Precedence of Parameters"},{"location":"gs/storage/","text":"KVDB Storage The following provides an overview of important KVDB storage concepts. Media Classes The KVDB storage model is based on the concept of media classes, where each media class represents a tier of storage. HSE currently defines two media classes for a KVDB: Capacity , which is required and represents the bulk (bottom) tier of storage Staging , which is optional and represents a faster (higher) tier of storage Each media class for a KVDB is simply a directory in a file system. For example, a KVDB could be configured with a capacity directory in a file system on cost-optimized SATA QLC SSDs, and a staging directory in a file system on performance-optimized NVMe TLC SSDs. The data for a KVDB is stored as a set of files in each of its capacity and (optional) staging directories. KVDB and Runtime Homes Every KVDB has a home directory with files storing its metadata. A KVDB home directory may also contain an optional kvdb.conf file with user-defined KVDB and KVS parameter settings. HSE also defines a runtime home directory for an application embedding HSE. This runtime home may contain an optional hse.conf file with user-defined global parameter settings. Info Most applications will use a single KVDB with only the capacity media class configured. In this common case, the KVDB and runtime homes can be the same directory, and this directory can contain the capacity media class.","title":"KVDB Storage"},{"location":"gs/storage/#kvdb-storage","text":"The following provides an overview of important KVDB storage concepts.","title":"KVDB Storage"},{"location":"gs/storage/#media-classes","text":"The KVDB storage model is based on the concept of media classes, where each media class represents a tier of storage. HSE currently defines two media classes for a KVDB: Capacity , which is required and represents the bulk (bottom) tier of storage Staging , which is optional and represents a faster (higher) tier of storage Each media class for a KVDB is simply a directory in a file system. For example, a KVDB could be configured with a capacity directory in a file system on cost-optimized SATA QLC SSDs, and a staging directory in a file system on performance-optimized NVMe TLC SSDs. The data for a KVDB is stored as a set of files in each of its capacity and (optional) staging directories.","title":"Media Classes"},{"location":"gs/storage/#kvdb-and-runtime-homes","text":"Every KVDB has a home directory with files storing its metadata. A KVDB home directory may also contain an optional kvdb.conf file with user-defined KVDB and KVS parameter settings. HSE also defines a runtime home directory for an application embedding HSE. This runtime home may contain an optional hse.conf file with user-defined global parameter settings. Info Most applications will use a single KVDB with only the capacity media class configured. In this common case, the KVDB and runtime homes can be the same directory, and this directory can contain the capacity media class.","title":"KVDB and Runtime Homes"},{"location":"gs/sysreqs/","text":"System Requirements The following are system requirements for running HSE applications. Hardware Hardware requirements are largely dictated by the application embedding HSE and the amount of data stored. The following are general guidelines. CPU : x86_64; 3.0 GHz or higher; 16 threads or more Memory : 32 GB or more Storage : SSD volumes only ; use NVMe for best performance If KVDB storage is configured on multiple devices, such as when using XFS with LVM, performance can be significantly improved by balancing these devices across NUMA nodes. Tools like lstopo can be helpful in creating and verifying a balanced configuration. Operating System HSE should work with most modern Linux\u00ae 64-bit operating system distributions. We have run HSE on the following. Red Hat\u00ae Enterprise Linux\u00ae 8 (RHEL 8) Ubuntu\u00ae 18.04 File System HSE requires the following file system features. fallocate(2) with modes zero (0), FALLOC_FL_PUNCH_HOLE , FALLOC_FL_KEEP_SIZE openat(2) with flag O_DIRECT Several common file systems support these features, including XFS and ext4. For most HSE applications we recommend using XFS. Virtual Memory HSE performance and quality of service (QoS) can be significantly improved by configuring huge pages . We recommend setting vm.nr_hugepages=256 on your system.","title":"System Requirements"},{"location":"gs/sysreqs/#system-requirements","text":"The following are system requirements for running HSE applications.","title":"System Requirements"},{"location":"gs/sysreqs/#hardware","text":"Hardware requirements are largely dictated by the application embedding HSE and the amount of data stored. The following are general guidelines. CPU : x86_64; 3.0 GHz or higher; 16 threads or more Memory : 32 GB or more Storage : SSD volumes only ; use NVMe for best performance If KVDB storage is configured on multiple devices, such as when using XFS with LVM, performance can be significantly improved by balancing these devices across NUMA nodes. Tools like lstopo can be helpful in creating and verifying a balanced configuration.","title":"Hardware"},{"location":"gs/sysreqs/#operating-system","text":"HSE should work with most modern Linux\u00ae 64-bit operating system distributions. We have run HSE on the following. Red Hat\u00ae Enterprise Linux\u00ae 8 (RHEL 8) Ubuntu\u00ae 18.04","title":"Operating System"},{"location":"gs/sysreqs/#file-system","text":"HSE requires the following file system features. fallocate(2) with modes zero (0), FALLOC_FL_PUNCH_HOLE , FALLOC_FL_KEEP_SIZE openat(2) with flag O_DIRECT Several common file systems support these features, including XFS and ext4. For most HSE applications we recommend using XFS.","title":"File System"},{"location":"gs/sysreqs/#virtual-memory","text":"HSE performance and quality of service (QoS) can be significantly improved by configuring huge pages . We recommend setting vm.nr_hugepages=256 on your system.","title":"Virtual Memory"},{"location":"res/benchmarking/","text":"Benchmarking Tips The following are tips for benchmarking HSE applications. System Requirements The system running the HSE application should meet the specified requirements . In particular: Configure huge pages as documented Balance storage devices across NUMA nodes when applicable Configuration Parameters In the HSE application or kvdb.conf file, provide appropriate values for at least the following parameters. KVDB parameters : throttle_init_policy with a value determined by kvdb_profile dur_intvl_ms with a value appropriate for the application dur_mclass=staging if a staging media class is configured for the KVDB KVS parameters : value_compression=lz4 for all KVSs, unless the application performs its own value compression","title":"Benchmarking Tips"},{"location":"res/benchmarking/#benchmarking-tips","text":"The following are tips for benchmarking HSE applications.","title":"Benchmarking Tips"},{"location":"res/benchmarking/#system-requirements","text":"The system running the HSE application should meet the specified requirements . In particular: Configure huge pages as documented Balance storage devices across NUMA nodes when applicable","title":"System Requirements"},{"location":"res/benchmarking/#configuration-parameters","text":"In the HSE application or kvdb.conf file, provide appropriate values for at least the following parameters. KVDB parameters : throttle_init_policy with a value determined by kvdb_profile dur_intvl_ms with a value appropriate for the application dur_mclass=staging if a staging media class is configured for the KVDB KVS parameters : value_compression=lz4 for all KVSs, unless the application performs its own value compression","title":"Configuration Parameters"},{"location":"res/community/","text":"Community Resources are available to report a bug, ask a question, provide feedback on how we can improve HSE, or contribute to development. Report a Bug You can file an issue to report an bug. In doing so, please provide all the information requested in the issue template. Ask a Question You can ask a question in the HSE discussions forum using the Q&A category. Participate in Discussions You can discuss all aspects of HSE in the discussions forum using the following categories. Q&A to ask a question Ideas to discuss a new feature or enhancement Show and tell to talk about your experiences with HSE General to discuss all other aspects of the project Contribute to Development We welcome your contributions to the HSE project. As a starting point, see CONTRIBUTING.md in the hse repo for instructions on how to participate.","title":"Community"},{"location":"res/community/#community","text":"Resources are available to report a bug, ask a question, provide feedback on how we can improve HSE, or contribute to development.","title":"Community"},{"location":"res/community/#report-a-bug","text":"You can file an issue to report an bug. In doing so, please provide all the information requested in the issue template.","title":"Report a Bug"},{"location":"res/community/#ask-a-question","text":"You can ask a question in the HSE discussions forum using the Q&A category.","title":"Ask a Question"},{"location":"res/community/#participate-in-discussions","text":"You can discuss all aspects of HSE in the discussions forum using the following categories. Q&A to ask a question Ideas to discuss a new feature or enhancement Show and tell to talk about your experiences with HSE General to discuss all other aspects of the project","title":"Participate in Discussions"},{"location":"res/community/#contribute-to-development","text":"We welcome your contributions to the HSE project. As a starting point, see CONTRIBUTING.md in the hse repo for instructions on how to participate.","title":"Contribute to Development"},{"location":"res/relnotes/","text":"Releases Info Coming soon with the release of HSE 2.0.0.","title":"Releases"},{"location":"res/relnotes/#releases","text":"Info Coming soon with the release of HSE 2.0.0.","title":"Releases"}]}